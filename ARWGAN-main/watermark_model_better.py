import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision
import numpy as np
import math
from PIL import Image
import random
import os
import csv
import time
from pathlib import Path

# ------------------- æ”¹é€²å»ºè­°èªªæ˜ï¼ˆä¿®å¾©ç‰ˆï¼‰-------------------
# æ­¤ç¨‹å¼ç¢¼å¯¦ç¾æ”¹é€²ç‰ˆæ•¸å­—æ°´å°æ¨¡å‹ï¼ˆå·²ä¿®å¾©é—œéµ bugï¼‰ï¼š
# 1. Encoder: ResNet-like + CBAM attention (Channel + Spatial)
# 2. Noise Layer: ä½¿ç”¨ JPEG å™ªè²æ¨¡æ“¬å™¨ + å¤šç¨®æ”»æ“Š
# 3. Decoder: ResNet-Style CNN åˆ†é¡å™¨ï¼ˆç´”ä¸‹æ¡æ¨£ï¼Œç„¡ U-Netï¼‰
# 4. Discriminator: PatchGAN é¢¨æ ¼
# 5. Loss: MSE + SSIM + VGGæ„ŸçŸ¥æå¤± + BCE + WGAN-GP
# 6. ä¿®å¾©é …ç›®:
#    - SpatialAttention é‚è¼¯éŒ¯èª¤ï¼ˆå·²ä¿®æ­£ï¼‰
#    - Encoder è¼¸å‡ºå±¤è¨­è¨ˆï¼ˆæ”¹ç”¨ 1x1 convï¼‰
#    - JPEG æ¨¡æ“¬å¯¦ç¾ï¼ˆä½¿ç”¨é«˜æ–¯å™ªè²æ¨¡æ“¬ï¼‰
#    - åŠ å…¥å®Œæ•´è¨“ç·´æ¡†æ¶ï¼ˆcheckpointã€é©—è­‰é›†ã€TensorBoardï¼‰
# é‹è¡Œ: python watermark_model_better.py --train --epochs 100 --batch 16
# ------------------------------------------------------------

# CBAM Attention Module (æ”¹é€²æ³¨æ„åŠ›æ©Ÿåˆ¶)
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc1 = nn.Linear(in_planes, in_planes // ratio, bias=False)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(in_planes // ratio, in_planes, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x).view(x.size(0), -1))))
        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x).view(x.size(0), -1))))
        out = avg_out + max_out
        return self.sigmoid(out).view(x.size(0), x.size(1), 1, 1) * x

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # ä¿®å¾©ï¼šä¿å­˜åŸå§‹è¼¸å…¥
        x_input = x
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        concat = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(concat))
        # ä¿®å¾©ï¼šç”¨ attention mask ä¹˜ä»¥åŸå§‹è¼¸å…¥ï¼Œè€Œé conv å¾Œçš„çµæœ
        return attention * x_input

class CBAM(nn.Module):
    def __init__(self, channels, ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.ca = ChannelAttention(channels, ratio)
        self.sa = SpatialAttention(kernel_size)

    def forward(self, x):
        x = self.ca(x)
        x = self.sa(x)
        return x

# Encoder with Dense Connections and CBAM (æ”¹é€²ç·¨ç¢¼å™¨)
class Encoder(nn.Module):
    def __init__(self, watermark_bits=64):
        super(Encoder, self).__init__()
        self.watermark_bits = watermark_bits
        # Initial conv to extract features
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        
        # Dense block layers
        self.dense1 = nn.Conv2d(64, 32, kernel_size=3, padding=1)
        self.dense2 = nn.Conv2d(96, 32, kernel_size=3, padding=1)  # 64+32=96
        self.dense3 = nn.Conv2d(128, 32, kernel_size=3, padding=1) # 96+32=128
        self.dense4 = nn.Conv2d(160, 64, kernel_size=3, padding=1) # 128+32=160, output 64
        
        # CBAM attention
        self.cbam = CBAM(64)
        
        # Watermark embedding
        self.wm_embed = nn.Conv2d(watermark_bits, 64, kernel_size=1)  # Embed watermark channels
        
        # ============================================================
        # Residual è¼¸å‡ºå±¤ï¼ˆç§»é™¤ Tanhï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼‰
        # ============================================================
        # Shock Therapyï¼šå¢å¤§åˆå§‹è¨Šè™Ÿå¼·åº¦ï¼Œè®“ Decoder åœ¨è¨“ç·´åˆæœŸèƒ½å¾èƒŒæ™¯ä¸­æå–ç‰¹å¾µ
        self.to_rgb = nn.Conv2d(128, 3, kernel_size=1)
        nn.init.normal_(self.to_rgb.weight, mean=0, std=0.1)   # std=0.1 å¢åŠ åˆå§‹å™ªè²å¼·åº¦
        if self.to_rgb.bias is not None:
            nn.init.zeros_(self.to_rgb.bias)
        
        # Residual scalingï¼š0.5 é™åˆ¶å–®éšæ®µæ”¹å‹•å¹…åº¦ï¼Œé¿å… Phase 1 ç•«è³ªå´©æ½°ï¼›GAN å•Ÿç”¨å¾Œå¯è¦–éœ€è¦èª¿é«˜
        self.residual_scale = 0.5
        
    def forward(self, image, watermark):
        # image: (B,3,H,W), watermark: (B, bits) binary tensor
        x = self.relu(self.bn1(self.conv1(image)))
        
        # Dense connections
        d1 = self.relu(self.dense1(x))
        x = torch.cat([x, d1], dim=1)
        d2 = self.relu(self.dense2(x))
        x = torch.cat([x, d2], dim=1)
        d3 = self.relu(self.dense3(x))
        x = torch.cat([x, d3], dim=1)
        d4 = self.relu(self.dense4(x))
        
        # Apply CBAM attention to guide embedding
        attended = self.cbam(d4)
        
        # Prepare watermark: repeat to match image size, create channels
        B, _, H, W = image.shape
        wm_repeated = watermark.unsqueeze(2).unsqueeze(3).repeat(1,1,H,W)  # (B,bits,H,W)
        wm_embedded = self.wm_embed(wm_repeated.float())  # (B,64,H,W)
        
        # ç›´æ¥ Concatï¼ˆç§»é™¤ BatchNormï¼‰
        fused = torch.cat([attended, wm_embedded], dim=1)  # (B,128,H,W)
        
        # Scalingï¼šæ§åˆ¶æ°´å°å¼·åº¦ï¼ˆç§»é™¤ Tanh é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼‰
        residual = self.to_rgb(fused) * self.residual_scale
        watermarked = image + residual
        return torch.clamp(watermarked, 0, 1)

# JPEG å™ªè²æ¨¡æ“¬å™¨ï¼ˆç°¡åŒ–ç‰ˆï¼Œä½¿ç”¨é«˜æ–¯å™ªè²æ¨¡æ“¬å£“ç¸®æ•ˆæœï¼‰
class JPEGNoiseSimulator(nn.Module):
    """
    ç°¡åŒ–çš„ JPEG å£“ç¸®æ¨¡æ“¬å™¨
    æ³¨æ„ï¼šé€™ä¸æ˜¯çœŸæ­£çš„ DCT-based JPEGï¼Œè€Œæ˜¯ä½¿ç”¨é«˜æ–¯å™ªè²ä¾†æ¨¡æ“¬å£“ç¸®å¤±çœŸ
    å„ªé»ï¼šå¯å¾®åˆ†ã€è¨ˆç®—å¿«é€Ÿ
    ç¼ºé»ï¼šä¸å®Œå…¨æ¨¡æ“¬çœŸå¯¦ JPEG çš„å¡Šç‹€å½å½±
    """
    def __init__(self, device):
        super(JPEGNoiseSimulator, self).__init__()
        self.device = device
    
    def forward(self, x, quality_factor=50):
        """
        Args:
            x: è¼¸å…¥åœ–åƒ [B, C, H, W]ï¼Œç¯„åœ [0, 1]
            quality_factor: JPEG å“è³ª (1-100)ï¼Œè¶Šä½å™ªè²è¶Šå¤§
        Returns:
            æ¨¡æ“¬å£“ç¸®å¾Œçš„åœ–åƒ
        """
        # æ ¹æ“šå“è³ªå› å­è¨ˆç®—å™ªè²å¼·åº¦
        # quality=100 -> noise_std=0.02, quality=0 -> noise_std=0.10
        quality_scale = (100 - quality_factor) / 100.0
        noise_std = 0.02 + quality_scale * 0.08
        
        # æ·»åŠ é«˜æ–¯å™ªè²æ¨¡æ“¬å£“ç¸®å¤±çœŸ
        noised = x + torch.randn_like(x) * noise_std
        
        return torch.clamp(noised, 0, 1)

# Noise Layer (æ¨¡æ“¬å„ç¨®æ”»æ“Šï¼šé«˜æ–¯å™ªè²ã€JPEG å£“ç¸®ã€è£å‰ªã€Dropoutã€ç¸®æ”¾)
class NoiseLayer(nn.Module):
    def __init__(self, device, attacks=['gaussian', 'jpeg', 'crop', 'dropout', 'resize']):
        super(NoiseLayer, self).__init__()
        self.attacks = attacks
        self.device = device
        # JPEG å™ªè²æ¨¡æ“¬å™¨ï¼ˆä½¿ç”¨é«˜æ–¯å™ªè²æ¨¡æ“¬å£“ç¸®æ•ˆæœï¼‰
        self.jpeg_simulator = JPEGNoiseSimulator(device)
        # éšæ®µå¼ Warm-upï¼šNoise æ”»æ“Šåœ¨ GAN ä¹‹å‰å•Ÿç”¨ï¼Œè®“æ¨¡å‹å…ˆå­¸ç¿’æŠ—æ”»æ“Š
        self.warmup_epochs = 5   # Noise åœ¨ Epoch 5 é–‹å§‹å•Ÿç”¨
        self.noise_ramp_epochs = 10  # Epoch 5â†’15 æ”»æ“Šæ©Ÿç‡å¾ 0 ç·šæ€§å¢è‡³ 1ï¼Œé¿å…ä¸€æ¬¡å…¨é–‹å°è‡´ BER å´©æ½°
        self.current_epoch = 0
        self.enable_attacks = False
        self.attack_prob = 0.0   # ç•¶å‰ epoch çš„æ”»æ“Šæ©Ÿç‡ï¼ˆæ¼¸é€²ç”¨ï¼‰

    def gaussian_noise(self, x, std=0.05):
        noise = torch.randn_like(x) * std
        return torch.clamp(x + noise, 0, 1)

    def jpeg_compression(self, x, quality=50):
        # ä½¿ç”¨ JPEG å™ªè²æ¨¡æ“¬å™¨
        return self.jpeg_simulator(x, quality_factor=quality)

    def crop(self, x, ratio=0.1):
        # Random crop and pad back
        B, C, H, W = x.shape
        crop_h = int(H * ratio)
        crop_w = int(W * ratio)
        start_h = random.randint(0, max(1, H - crop_h))
        start_w = random.randint(0, max(1, W - crop_w))
        cropped = x[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w]
        padded = F.pad(cropped, (start_w, W - start_w - crop_w, start_h, H - start_h - crop_h), mode='constant', value=0)
        return padded

    def dropout(self, x, original_image, ratio=0.1):
        # Dropout block and replace with original block
        B, C, H, W = x.shape
        block_h = max(1, int(H * ratio))
        block_w = max(1, int(W * ratio))
        start_h = random.randint(0, max(1, H - block_h))
        start_w = random.randint(0, max(1, W - block_w))
        x_clone = x.clone()
        x_clone[:, :, start_h:start_h+block_h, start_w:start_w+block_w] = original_image[:, :, start_h:start_h+block_h, start_w:start_w+block_w]
        return x_clone

    def resize(self, x, scale=0.5):
        return F.interpolate(F.interpolate(x, scale_factor=scale, mode='bicubic', align_corners=False), 
                           size=x.shape[2:], mode='bicubic', align_corners=False)

    def set_epoch(self, epoch):
        """è¨­ç½®ç•¶å‰ epochï¼Œç”¨æ–¼ Warm-up èˆ‡æ¼¸é€²æ”»æ“Šæ©Ÿç‡"""
        self.current_epoch = epoch
        # å‰ warmup_epochs å€‹ epochs é—œé–‰æ”»æ“Š
        self.enable_attacks = (epoch >= self.warmup_epochs)
        # æ¼¸é€²ï¼šepoch 5â†’5+noise_ramp_epochs æ”»æ“Šæ©Ÿç‡å¾ 0 ç·šæ€§å¢è‡³ 1ï¼Œé¿å…ä¸€æ¬¡å…¨é–‹ BER å´©æ½°
        if epoch < self.warmup_epochs:
            self.attack_prob = 0.0
        else:
            ramp = (epoch - self.warmup_epochs) / max(1, self.noise_ramp_epochs)
            self.attack_prob = min(1.0, ramp)
    
    def forward(self, x, original_image=None):
        # Warm-up æ©Ÿåˆ¶ï¼šå‰ warmup_epochs å€‹ epochs ç›´æ¥è¿”å›åŸå§‹è¼¸å…¥
        if not self.enable_attacks:
            return x
        # æ¼¸é€²æ”»æ“Šï¼šä»¥ attack_prob æ©Ÿç‡æ–½åŠ æ”»æ“Šï¼Œå…¶é¤˜æ™‚é–“å‚³å›åŸåœ–ï¼Œè®“ Decoder æœ‰æ©Ÿæœƒåœ¨ã€Œéƒ¨åˆ†æ”»æ“Šã€ä¸‹å­¸ç¿’
        if random.random() >= self.attack_prob:
            return x
        attack = random.choice(self.attacks)
        if attack == 'gaussian':
            return self.gaussian_noise(x)
        elif attack == 'jpeg':
            return self.jpeg_compression(x)
        elif attack == 'crop':
            return self.crop(x)
        elif attack == 'dropout' and original_image is not None:
            return self.dropout(x, original_image)
        elif attack == 'resize':
            return self.resize(x)
        return x  # No attack or fallback

# ============================================================
# Decoder (ResNet-Style CNN åˆ†é¡å™¨)
# ============================================================
# è¨­è¨ˆç†å¿µï¼š
#   - ç§»é™¤ U-Net çš„ Upsampling å’Œ Skip Connection
#   - ç´”ä¸‹æ¡æ¨£ CNNï¼Œå°ˆç‚º 64-bit åˆ†é¡ä»»å‹™è¨­è¨ˆ
#   - ç°¡å–®ã€ç›´æ¥ã€æ¢¯åº¦æµå‹•é †æš¢
# ============================================================
class Decoder(nn.Module):
    def __init__(self, watermark_bits=64):
        super(Decoder, self).__init__()
        self.watermark_bits = watermark_bits
        
        # Block 1: 3 -> 64, 256x256 (stride=1, ä¿æŒå°ºå¯¸)
        self.block1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )
        
        # Block 2: 64 -> 64, 256 -> 128 (stride=2)
        self.block2 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )
        
        # Block 3: 64 -> 128, 128 -> 64 (stride=2)
        self.block3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )
        
        # Block 4: 128 -> 256, 64 -> 32 (stride=2)
        self.block4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Block 5: 256 -> 512, 32 -> 16 (stride=2)
        self.block5 = nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
        )
        
        # Global Average Pooling: 16x16 -> 1x1
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # Output Head: 512 -> watermark_bits
        self.fc = nn.Linear(512, watermark_bits)
        
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # é€£çºŒä¸‹æ¡æ¨£
        x = self.block1(x)   # (B, 64, 256, 256)
        x = self.block2(x)   # (B, 64, 128, 128)
        x = self.block3(x)   # (B, 128, 64, 64)
        x = self.block4(x)   # (B, 256, 32, 32)
        x = self.block5(x)   # (B, 512, 16, 16)
        
        # Global Aggregation
        x = self.global_pool(x)  # (B, 512, 1, 1)
        x = x.view(x.size(0), -1)  # (B, 512)
        
        # Output
        logits = self.fc(x)  # (B, watermark_bits)
        extracted = (self.sigmoid(logits) > 0.5).float()
        
        return extracted, logits

# Discriminator (PatchGAN for WGAN-GP)
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 4, stride=2, padding=1)
        self.conv2 = nn.Conv2d(64, 128, 4, stride=2, padding=1)
        self.conv3 = nn.Conv2d(128, 256, 4, stride=2, padding=1)
        self.conv4 = nn.Conv2d(256, 512, 4, stride=2, padding=1)
        self.conv5 = nn.Conv2d(512, 1, 4, stride=1, padding=0)  # Output scalar per patch

    def forward(self, x):
        x = F.leaky_relu(self.conv1(x), 0.2)
        x = F.leaky_relu(self.conv2(x), 0.2)
        x = F.leaky_relu(self.conv3(x), 0.2)
        x = F.leaky_relu(self.conv4(x), 0.2)
        return self.conv5(x).mean()  # Global average for scalar output

# VGG æ„ŸçŸ¥æå¤±ï¼ˆå„ªåŒ–ç‰ˆï¼šä½¿ç”¨ register_buffer é¿å…é‡è¤‡å»ºç«‹ tensorï¼‰
class VGGLoss(nn.Module):
    def __init__(self):
        super(VGGLoss, self).__init__()
        # é å…ˆè¨»å†Šç‚º bufferï¼ˆä¸ä½”æ¢¯åº¦ï¼Œè‡ªå‹•åŒæ­¥ deviceï¼‰
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
        
        vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)
        # ä½¿ç”¨ VGG16 çš„å‰ 3 å€‹ block
        self.vgg_layers = nn.Sequential(*list(vgg16.features.children())[:16])
        for param in self.vgg_layers.parameters():
            param.requires_grad = False

    def forward(self, x):
        # x: (B,3,H,W), ç¯„åœ [0,1]ï¼›VGG éœ€ ImageNet æ¨™æº–åŒ–
        # ç›´æ¥ä½¿ç”¨ bufferï¼Œç„¡éœ€é‡è¤‡å»ºç«‹ tensor
        x_norm = (x - self.mean) / self.std
        return self.vgg_layers(x_norm)

# SSIM Loss (for image quality)
# æ¨™æº– SSIM: (2*Î¼x*Î¼y + C1)(2*Ïƒxy + C2) / ((Î¼xÂ² + Î¼yÂ² + C1)(ÏƒxÂ² + ÏƒyÂ² + C2))ï¼Œå›å‚³ 1-SSIM ä½œç‚º loss
def ssim_loss(img1, img2):
    mu1 = F.avg_pool2d(img1, 11, stride=1, padding=5)
    mu2 = F.avg_pool2d(img2, 11, stride=1, padding=5)
    sigma1_sq = F.avg_pool2d(img1**2, 11, stride=1, padding=5) - mu1**2
    sigma2_sq = F.avg_pool2d(img2**2, 11, stride=1, padding=5) - mu2**2
    sigma12 = F.avg_pool2d(img1*img2, 11, stride=1, padding=5) - mu1*mu2
    C1, C2 = 0.01**2, 0.03**2
    # æ•¸å€¼ç©©å®šï¼šlocal variance ç†è«–ä¸Š â‰¥0ï¼Œæµ®é»èª¤å·®å¯èƒ½ç•¥è² ï¼Œclamp é¿å…ç•°å¸¸
    sigma1_sq = sigma1_sq.clamp(min=0)
    sigma2_sq = sigma2_sq.clamp(min=0)
    ssim = ((2*mu1*mu2 + C1) * (2*sigma12 + C2)) / ((mu1**2 + mu2**2 + C1) * (sigma1_sq + sigma2_sq + C2) + 1e-8)
    return 1 - ssim.mean()

# WGAN-GP Loss
def wgan_gp_loss(discriminator, real_imgs, fake_imgs, lambda_gp=10):
    batch_size = real_imgs.size(0)
    alpha = torch.rand(batch_size, 1, 1, 1).to(real_imgs.device).expand_as(real_imgs)
    interpolates = alpha * real_imgs + (1 - alpha) * fake_imgs
    interpolates.requires_grad_(True)
    disc_interpolates = discriminator(interpolates)
    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,
                                    grad_outputs=torch.ones_like(disc_interpolates),
                                    create_graph=True, retain_graph=True)[0]
    gradients = gradients.view(batch_size, -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp
    return gradient_penalty

# CSV è¨˜éŒ„å‡½æ•¸ï¼ˆé¡ä¼¼åŸå§‹ç¨‹å¼çš„ write_lossesï¼‰
def write_losses_to_csv(file_name, losses_dict, epoch, duration):
    """å°‡æå¤±å¯«å…¥ CSV æª”æ¡ˆ"""
    file_exists = os.path.exists(file_name) and os.path.getsize(file_name) > 0
    with open(file_name, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            # åªæœ‰åœ¨æª”æ¡ˆä¸å­˜åœ¨æˆ–ç‚ºç©ºæ™‚æ‰å¯«å…¥æ¨™é¡Œè¡Œ
            row_to_write = ['epoch'] + list(losses_dict.keys()) + ['duration']
            writer.writerow(row_to_write)
        # å¯«å…¥æ•¸æ“šè¡Œ
        row_to_write = [epoch] + ['{:.4f}'.format(v) for v in losses_dict.values()] + ['{:.0f}'.format(duration)]
        writer.writerow(row_to_write)

# Dataset (COCO example) â€” å›ºå®šè¼¸å‡ºå°ºå¯¸ï¼Œé¿å… DataLoader collate æ™‚å°ºå¯¸ä¸ä¸€è‡´
TARGET_IMAGE_SIZE = (256, 256)

class WatermarkDataset(Dataset):
    def __init__(self, root_dir='./data/coco/images/train2017', transform=None, watermark_bits=64):
        self.root_dir = root_dir
        self.transform = transform or transforms.Compose([
            transforms.Resize(TARGET_IMAGE_SIZE),
            transforms.ToTensor(),
        ])
        
        # é¦–å…ˆå˜—è©¦åœ¨æŒ‡å®šç›®éŒ„ä¸­æŸ¥æ‰¾åœ–ç‰‡
        if not os.path.exists(root_dir):
            raise ValueError(f"æ•¸æ“šé›†ç›®éŒ„ä¸å­˜åœ¨: {root_dir}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç›®éŒ„
        if not os.path.isdir(root_dir):
            raise ValueError(f"æŒ‡å®šçš„è·¯å¾‘ä¸æ˜¯ç›®éŒ„: {root_dir}")
        
        # å˜—è©¦å¤šç¨®æ–¹å¼æŸ¥æ‰¾åœ–ç‰‡æ–‡ä»¶
        self.image_list = []
        
        # æ–¹æ³•1: ç›´æ¥åœ¨æŒ‡å®šç›®éŒ„ä¸­æŸ¥æ‰¾
        # ç°¡åŒ–é‚è¼¯ï¼šä¸ä½¿ç”¨ os.path.islink/realpathï¼Œé¿å…å¤šé€²ç¨‹å•é¡Œ
        try:
            all_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            for f in all_files:
                img_path = os.path.join(root_dir, f)
                if os.path.exists(img_path):
                    self.image_list.append(f)
        except (OSError, PermissionError):
            pass
        
        # æ–¹æ³•2: å¦‚æœç›´æ¥ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦æœç´¢å¸¸è¦‹çš„å­ç›®éŒ„çµæ§‹
        if len(self.image_list) == 0:
            common_subdirs = [
                'train/images',
                'train',
                'images',
                'train2017',
                'val/images',
                'val',
            ]
            
            for subdir in common_subdirs:
                search_path = os.path.join(root_dir, subdir)
                if os.path.exists(search_path) and os.path.isdir(search_path):
                    try:
                        files = [f for f in os.listdir(search_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                        for f in files:
                            img_path = os.path.join(search_path, f)
                            if os.path.exists(img_path):
                                self.image_list.append(f)
                        
                        if len(self.image_list) > 0:
                            # æ›´æ–° root_dir ç‚ºæ‰¾åˆ°åœ–ç‰‡çš„ç›®éŒ„
                            self.root_dir = search_path
                            print(f"åœ¨å­ç›®éŒ„ {subdir} ä¸­æ‰¾åˆ°åœ–ç‰‡æ–‡ä»¶")
                            break
                    except (OSError, PermissionError):
                        continue
        
        # æ–¹æ³•3: éè¿´æœç´¢æ‰€æœ‰å­ç›®éŒ„ï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰
        if len(self.image_list) == 0:
            print(f"åœ¨ {root_dir} ä¸­æœªæ‰¾åˆ°åœ–ç‰‡ï¼Œé–‹å§‹éè¿´æœç´¢...")
            for root, dirs, files in os.walk(root_dir):
                for f in files:
                    if f.lower().endswith(('.jpg', '.jpeg', '.png')):
                        img_path = os.path.join(root, f)
                        if os.path.exists(img_path):
                            # ä¿å­˜ç›¸å°è·¯å¾‘
                            rel_path = os.path.relpath(img_path, root_dir)
                            self.image_list.append(rel_path)
                
                # å¦‚æœæ‰¾åˆ°è¶³å¤ çš„åœ–ç‰‡ï¼Œåœæ­¢æœç´¢
                if len(self.image_list) > 100:
                    break
        
        if len(self.image_list) == 0:
            raise ValueError(
                f"åœ¨ {root_dir} ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ï¼\n"
                f"è«‹æª¢æŸ¥æ•¸æ“šé›†æ˜¯å¦æ­£ç¢ºä¸‹è¼‰ï¼Œæˆ–ä½¿ç”¨ --data-dir æŒ‡å®šåŒ…å«åœ–ç‰‡çš„å…·é«”ç›®éŒ„ã€‚\n"
                f"å¸¸è¦‹çš„ç›®éŒ„çµæ§‹: data/coco2017/train/images æˆ– data/coco/images/train2017"
            )
        
        # æœ€çµ‚é©—è­‰ï¼šéæ¿¾æ‰ä»»ä½• None æˆ–ç„¡æ•ˆçš„è·¯å¾‘
        self.image_list = [f for f in self.image_list if f and isinstance(f, str) and len(f) > 0]
        
        if len(self.image_list) == 0:
            raise ValueError(f"éæ¿¾å¾Œæ²’æœ‰æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ï¼")
        
        print(f"æ‰¾åˆ° {len(self.image_list)} å€‹æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ï¼ˆåœ¨ {self.root_dir}ï¼‰")
        self.watermark_bits = watermark_bits

    def __len__(self):
        return len(self.image_list)

    def _ensure_size(self, tensor):
        """ç¢ºä¿å½±åƒå¼µé‡ç‚º (C, 256, 256)ï¼Œé¿å… DataLoader collate æ™‚å°ºå¯¸ä¸ä¸€è‡´ã€‚"""
        if tensor.shape[-2:] != TARGET_IMAGE_SIZE:
            tensor = F.interpolate(
                tensor.unsqueeze(0), size=TARGET_IMAGE_SIZE, mode='bilinear', align_corners=False
            ).squeeze(0)
        return tensor

    def __getitem__(self, idx):
        import random
        import torch
        from PIL import Image
        
        max_retries = 10
        
        for attempt in range(max_retries):
            try:
                # é¸æ“‡åœ–ç‰‡ç´¢å¼•ï¼ˆé¦–æ¬¡ä½¿ç”¨åŸå§‹ idxï¼Œé‡è©¦æ™‚ä½¿ç”¨éš¨æ©Ÿç´¢å¼•ï¼‰
                current_idx = idx if attempt == 0 else random.randint(0, len(self.image_list) - 1)
                img_file = self.image_list[current_idx]
                
                # ç¢ºä¿ img_file ç‚ºæœ‰æ•ˆå­—ä¸²
                if not isinstance(img_file, str):
                    img_file = str(img_file) if img_file is not None else ""
                
                if not img_file:
                    continue
                
                # æ§‹å»ºå®Œæ•´è·¯å¾‘
                if os.path.isabs(img_file):
                    img_path = img_file
                else:
                    img_path = os.path.join(self.root_dir, img_file)
                
                # ç¢ºä¿ img_path ç‚ºå­—ä¸²ï¼ˆå®‰å…¨æ€§æª¢æŸ¥ï¼‰
                if not isinstance(img_path, str):
                    img_path = str(img_path)
                
                # ç›´æ¥è®“ Image.open() è™•ç†è·¯å¾‘ï¼Œç§»é™¤æ‰€æœ‰ os.path.realpath/islink æª¢æŸ¥
                # é€™æ¨£å¯ä»¥é¿å…åœ¨å¤šé€²ç¨‹ DataLoader ä¸­è§¸ç™¼ posixpath.py çš„ UnboundLocalError
                image = Image.open(img_path)
                image = image.convert('RGB')
                image.load()  # ç¢ºä¿åœ–åƒæ•¸æ“šè¢«åŠ è¼‰åˆ°è¨˜æ†¶é«”ä¸­
                
                # é©—è­‰åœ–ç‰‡æ˜¯å¦æœ‰æ•ˆ
                if image.size is None or image.size[0] <= 0 or image.size[1] <= 0:
                    raise ValueError(f"ç„¡æ•ˆçš„åœ–ç‰‡å°ºå¯¸")
                
                # æ‡‰ç”¨ transform ä¸¦ç¢ºä¿å°ºå¯¸ä¸€è‡´
                image_tensor = self.transform(image)
                image_tensor = self._ensure_size(image_tensor)
                
                # Random binary watermark
                watermark = torch.randint(0, 2, (self.watermark_bits,)).float()
                return image_tensor, watermark
                
            except Exception:
                # å¦‚æœ Image.open æˆ–ä»»ä½•æ­¥é©Ÿå¤±æ•—ï¼Œç›´æ¥é€²å…¥é‡è©¦é‚è¼¯
                continue
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œä½¿ç”¨é»‘è‰²åœ–ç‰‡ä½œç‚ºå¾Œå‚™
        fallback_image = Image.new('RGB', TARGET_IMAGE_SIZE, color=(0, 0, 0))
        image_tensor = self.transform(fallback_image)
        image_tensor = self._ensure_size(image_tensor)
        watermark = torch.randint(0, 2, (self.watermark_bits,)).float()
        return image_tensor, watermark

# Training Function (æ”¹é€²ç‰ˆï¼šåŠ å…¥é©—è­‰é›†ã€checkpointã€å­¸ç¿’ç‡èª¿åº¦)
# æ³¨æ„ï¼šlr åƒæ•¸å·²æ£„ç”¨ï¼Œä½¿ç”¨å›ºå®šçš„å·®åˆ†å­¸ç¿’ç‡ï¼ˆEncoder=1e-4, Decoder=1e-3ï¼‰
def train_model(epochs=100, batch_size=16, lr=None, device='cuda', 
                save_dir='./checkpoints_improved', use_vgg=True, resume_from_checkpoint=None,
                data_dir=None):
    # å‰µå»ºä¿å­˜ç›®éŒ„
    save_dir = Path(save_dir)
    save_dir.mkdir(exist_ok=True, parents=True)
    
    # CSV æª”æ¡ˆè·¯å¾‘
    train_csv_path = save_dir / 'train.csv'
    validation_csv_path = save_dir / 'validation.csv'
    
    # è‡ªå‹•æª¢æ¸¬æ•¸æ“šé›†è·¯å¾‘
    if data_dir is None:
        # å˜—è©¦å¤šå€‹å¯èƒ½çš„æ•¸æ“šé›†è·¯å¾‘
        possible_paths = [
            './data/coco2017/train/images',
            './data/coco/images/train2017',
            './data/train',
            './data/coco/train',
        ]
        data_dir = None
        for path in possible_paths:
            if os.path.exists(path) and os.path.isdir(path):
                # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ï¼ˆåŒ…æ‹¬ç¬¦è™Ÿé€£çµï¼‰
                try:
                    files = [f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                    # å¦‚æœæœ‰åœ–ç‰‡æ–‡ä»¶ï¼Œä½¿ç”¨é€™å€‹è·¯å¾‘
                    if len(files) > 0:
                        data_dir = path
                        total_files = len(files)
                        print(f"è‡ªå‹•æª¢æ¸¬åˆ°æ•¸æ“šé›†è·¯å¾‘: {data_dir} (æ‰¾åˆ° {total_files} å€‹åœ–ç‰‡æ–‡ä»¶)")
                        break
                except (OSError, PermissionError) as e:
                    # å¦‚æœç„¡æ³•è®€å–ç›®éŒ„ï¼Œè·³é
                    continue
        
        if data_dir is None:
            raise ValueError(
                f"ç„¡æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ•¸æ“šé›†ï¼è«‹ä½¿ç”¨ --data-dir åƒæ•¸æŒ‡å®šæ•¸æ“šé›†è·¯å¾‘ã€‚\n"
                f"å˜—è©¦çš„è·¯å¾‘: {possible_paths}"
            )
    else:
        if not os.path.exists(data_dir):
            raise ValueError(f"æŒ‡å®šçš„æ•¸æ“šé›†è·¯å¾‘ä¸å­˜åœ¨: {data_dir}")
        print(f"ä½¿ç”¨æŒ‡å®šçš„æ•¸æ“šé›†è·¯å¾‘: {data_dir}")
    
    # è³‡æ–™é›†
    dataset = WatermarkDataset(root_dir=data_dir)
    # åˆ†å‰²è¨“ç·´/é©—è­‰é›† (90/10)
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # DataLoader é…ç½®ï¼šnum_workers=4 åŠ é€Ÿè¼‰å…¥ï¼Œä½¿ç”¨ spawn å•Ÿå‹•æ–¹å¼é™ä½ segfault é¢¨éšª
    num_workers = 4
    pin_memory = torch.cuda.is_available()
    use_persistent_workers = num_workers > 0  # é¿å…æ¯å€‹ epoch é‡å•Ÿ worker
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=use_persistent_workers,
        prefetch_factor=2 if num_workers > 0 else None
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=use_persistent_workers,
        prefetch_factor=2 if num_workers > 0 else None
    )
    print(f"DataLoader é…ç½®: num_workers={num_workers}, pin_memory={pin_memory}, persistent_workers={use_persistent_workers}")

    # æ¨¡å‹
    encoder = Encoder().to(device)
    noise_layer = NoiseLayer(device).to(device)
    decoder = Decoder().to(device)
    discriminator = Discriminator().to(device)
    
    # VGG Lossï¼ˆå¯é¸ï¼‰
    vgg_loss_fn = VGGLoss().to(device) if use_vgg else None

    # å„ªåŒ–å™¨ï¼ˆå·®åˆ†å­¸ç¿’ç‡ï¼‰
    # Encoder: 1e-4 (ç©©å®šç•«è³ªï¼Œé¿å…åœ–åƒçµæ§‹å´©æ½°)
    # Decoder: 1e-3 (åŠ é€Ÿæ”¶æ–‚ï¼Œå¿«é€Ÿå­¸ç¿’æµ®æ°´å°æå–)
    opt_gen = optim.Adam([
        {'params': encoder.parameters(), 'lr': 1e-4},
        {'params': decoder.parameters(), 'lr': 1e-3}
    ], betas=(0.5, 0.999))
    opt_disc = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))  # èˆ‡ Encoder ä¸€è‡´
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler_gen = optim.lr_scheduler.StepLR(opt_gen, step_size=30, gamma=0.5)
    scheduler_disc = optim.lr_scheduler.StepLR(opt_disc, step_size=30, gamma=0.5)
    
    # æå¤±å‡½æ•¸
    mse_loss = nn.MSELoss()
    bce_loss = nn.BCEWithLogitsLoss()
    
    # å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´ï¼ˆåŠ å…¥ç•°å¸¸è™•ç†ï¼‰
    start_epoch = 0
    best_val_ber = float('inf')
    
    if resume_from_checkpoint is not None and Path(resume_from_checkpoint).exists():
        try:
            print(f"\nå¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´: {resume_from_checkpoint}")
            checkpoint = torch.load(resume_from_checkpoint, map_location=device)
            
            # é©—è­‰ checkpoint å®Œæ•´æ€§
            required_keys = ['encoder_state_dict', 'decoder_state_dict', 'discriminator_state_dict', 'epoch']
            missing_keys = [k for k in required_keys if k not in checkpoint]
            if missing_keys:
                raise KeyError(f"Checkpoint ç¼ºå°‘å¿…è¦çš„éµ: {missing_keys}")
            
            # è¼‰å…¥æ¨¡å‹æ¬Šé‡ï¼ˆä½¿ç”¨ strict=False å®¹å¿éƒ¨åˆ†ä¸åŒ¹é…ï¼‰
            encoder.load_state_dict(checkpoint['encoder_state_dict'], strict=False)
            decoder.load_state_dict(checkpoint['decoder_state_dict'], strict=False)
            discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=False)
            
            # è¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹
            if 'opt_gen_state_dict' in checkpoint:
                try:
                    opt_gen.load_state_dict(checkpoint['opt_gen_state_dict'])
                except Exception as e:
                    print(f"âš ï¸  å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥å¤±æ•—ï¼Œå°‡é‡æ–°åˆå§‹åŒ–: {e}")
            if 'opt_disc_state_dict' in checkpoint:
                try:
                    opt_disc.load_state_dict(checkpoint['opt_disc_state_dict'])
                except Exception as e:
                    print(f"âš ï¸  Discriminator å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥å¤±æ•—: {e}")
            
            # è¼‰å…¥å­¸ç¿’ç‡èª¿åº¦å™¨ç‹€æ…‹
            if 'scheduler_gen_state_dict' in checkpoint:
                try:
                    scheduler_gen.load_state_dict(checkpoint['scheduler_gen_state_dict'])
                except Exception:
                    pass
            if 'scheduler_disc_state_dict' in checkpoint:
                try:
                    scheduler_disc.load_state_dict(checkpoint['scheduler_disc_state_dict'])
                except Exception:
                    pass
            
            # æ¢å¾© epoch å’Œæœ€ä½³ BER
            start_epoch = checkpoint['epoch'] + 1
            if 'best_val_ber' in checkpoint:
                best_val_ber = checkpoint['best_val_ber']
            
            print(f"âœ“ å·²æ¢å¾©åˆ° Epoch {start_epoch}")
            print(f"âœ“ æœ€ä½³é©—è­‰ BER: {best_val_ber:.4f}")
            if 'train_losses' in checkpoint:
                print(f"âœ“ ä¸Šæ¬¡è¨“ç·´æå¤±: {checkpoint['train_losses']}")
            if 'val_losses' in checkpoint:
                print(f"âœ“ ä¸Šæ¬¡é©—è­‰æå¤±: {checkpoint['val_losses']}")
            print()
            
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥ checkpoint å¤±æ•—: {e}")
            print("   å°‡å¾é ­é–‹å§‹è¨“ç·´...\n")
            start_epoch = 0
            best_val_ber = float('inf')
            
    elif resume_from_checkpoint is not None:
        print(f"âš ï¸  è­¦å‘Š: æª¢æŸ¥é»æ–‡ä»¶ä¸å­˜åœ¨: {resume_from_checkpoint}")
        print("   å°‡å¾é ­é–‹å§‹è¨“ç·´...\n")
    
    print(f"é–‹å§‹è¨“ç·´... è¨“ç·´é›†: {train_size}, é©—è­‰é›†: {val_size}")
    if start_epoch > 0:
        print(f"å¾ Epoch {start_epoch} ç¹¼çºŒè¨“ç·´ï¼Œç¸½å…± {epochs} epochs\n")

    # Sanity Check: Encoder to_rgb åˆå§‹åŒ– (std=0.1) èˆ‡ residual_scale=1.0
    with torch.no_grad():
        to_rgb_w = encoder.to_rgb.weight
        to_rgb_b = encoder.to_rgb.bias
        print(f"[Sanity Check] Encoder to_rgb.weight mean={to_rgb_w.mean().item():.6f}, std={to_rgb_w.std().item():.6f} (init std=0.1)")
        print(f"[Sanity Check] Encoder to_rgb.bias å¹³å‡: {to_rgb_b.mean().item():.6f} (æ‡‰ç‚º 0)")
        print(f"[Sanity Check] Encoder residual_scale: {encoder.residual_scale} (é™åˆ¶å–®éšæ®µæ”¹å‹•ï¼Œé¿å…ç•«è³ªå´©æ½°)\n")

    # ============================================================
    # éšæ®µå¼ Warm-up è¨­å®šï¼ˆé¿å…åŒæ™‚å•Ÿç”¨å¤šå€‹æ–°çµ„ä»¶å°è‡´è¨“ç·´éœ‡ç›ªï¼‰
    # ============================================================
    # Phase 1 (Epoch 0-4):  ç´”é€šè¨Šç³»çµ±ï¼ˆç„¡ GAN, ç„¡ Noiseï¼‰
    # Phase 2 (Epoch 5-14): åŠ å…¥ Noise æ”»æ“Šï¼ˆç„¡ GANï¼‰
    # Phase 3 (Epoch 15+):  å®Œæ•´è¨“ç·´ï¼ˆGAN + Noiseï¼‰
    # ============================================================
    NOISE_WARMUP_EPOCHS = 5   # Noise åœ¨ Epoch 5 é–‹å§‹å•Ÿç”¨
    GAN_WARMUP_EPOCHS = 15    # GAN åœ¨ Epoch 15 é–‹å§‹å•Ÿç”¨
    
    # æ—©åœæ©Ÿåˆ¶è¨­å®š
    early_stopping_patience = 15  # å®¹å¿ 15 å€‹ epochs ä¸æ”¹å–„
    patience_counter = 0
    
    for epoch in range(start_epoch, epochs):
        # ============= éšæ®µå¼ Warm-up æ©Ÿåˆ¶ =============
        noise_layer.set_epoch(epoch)
        gan_enabled = (epoch >= GAN_WARMUP_EPOCHS)
        
        # é¡¯ç¤ºç•¶å‰è¨“ç·´éšæ®µ
        if epoch < NOISE_WARMUP_EPOCHS:
            phase_name = "Phase 1: ç´”é€šè¨Šç³»çµ±"
            print(f"ğŸ”¥ {phase_name} (Epoch {epoch+1}/{NOISE_WARMUP_EPOCHS}): ç„¡ Noise, ç„¡ GAN")
        elif epoch < GAN_WARMUP_EPOCHS:
            phase_name = "Phase 2: æŠ—æ”»æ“Šè¨“ç·´"
            if epoch == NOISE_WARMUP_EPOCHS:
                print(f"\n{'='*60}")
                print(f"âœ… Phase 1 å®Œæˆï¼å¾ Epoch {epoch + 1} é–‹å§‹å•Ÿç”¨ Noise Layer æ”»æ“Š")
                print(f"{'='*60}\n")
            print(f"ğŸ”¥ {phase_name} (Epoch {epoch+1}): æœ‰ Noise, ç„¡ GAN")
        else:
            phase_name = "Phase 3: å®Œæ•´è¨“ç·´"
            if epoch == GAN_WARMUP_EPOCHS:
                print(f"\n{'='*60}")
                print(f"âœ… Phase 2 å®Œæˆï¼å¾ Epoch {epoch + 1} é–‹å§‹å•Ÿç”¨ GAN")
                print(f"{'='*60}\n")
        
        # ============= è¨“ç·´éšæ®µ =============
        encoder.train()
        decoder.train()
        discriminator.train()
        
        epoch_start_time = time.time()
        train_losses = {'g_loss': 0, 'd_loss': 0, 'ber': 0, 'psnr': 0}
        num_batches = 0
        
        for batch_idx, (images, watermarks) in enumerate(train_loader):
            images, watermarks = images.to(device), watermarks.to(device)
            
            # ============================================================
            # Train Discriminator (WGAN-GP) â€” åªåœ¨ Warm-up çµæŸå¾Œå•Ÿç”¨
            # ============================================================
            if gan_enabled:
                for _ in range(1):  # D è¨“ç·´æ¬¡æ•¸
                    opt_disc.zero_grad()
                    watermarked = encoder(images, watermarks)
                    d_real = discriminator(images)
                    d_fake = discriminator(watermarked.detach())
                    gp = wgan_gp_loss(discriminator, images, watermarked.detach())
                    d_loss = -d_real.mean() + d_fake.mean() + gp
                    d_loss.backward()
                    opt_disc.step()
            else:
                # Warm-up éšæ®µï¼šä¸è¨“ç·´ Discriminatorï¼Œd_loss è¨­ç‚º 0
                d_loss = torch.tensor(0.0, device=device)
            
            # Train Generator (Encoder + Decoder)
            opt_gen.zero_grad()
            watermarked = encoder(images, watermarks)
            noised = noise_layer(watermarked, original_image=images)
            extracted, logits = decoder(noised)
            
            # Losses
            mse_img_loss = mse_loss(watermarked, images)
            ssim_img_loss = ssim_loss(watermarked, images)
            wm_loss = bce_loss(logits, watermarks)
            
            # GAN Loss â€” åªåœ¨ Warm-up çµæŸå¾Œè¨ˆç®—
            if gan_enabled:
                g_gan_loss = -discriminator(watermarked).mean()
            else:
                g_gan_loss = torch.tensor(0.0, device=device)
            
            # VGG æ„ŸçŸ¥æå¤±
            if vgg_loss_fn is not None:
                # VGG éœ€è¦ 3 é€šé“ï¼Œç¯„åœ [0,1]
                vgg_real = vgg_loss_fn(images)
                vgg_fake = vgg_loss_fn(watermarked)
                vgg_perceptual_loss = mse_loss(vgg_fake, vgg_real)
                # ä¿®æ­£ï¼šssim_loss å·²ç¶“è¿”å› (1-ssim)ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨å³å¯
                img_loss = 0.5 * mse_img_loss + 0.3 * ssim_img_loss + 0.2 * vgg_perceptual_loss
            else:
                img_loss = mse_img_loss + ssim_img_loss
            
            # ============================================================
            # æå¤±æ¬Šé‡æ’ç¨‹
            # ============================================================
            # Phase 1 (Warm-up): å¼·è¿«å„ªå…ˆå„ªåŒ– BERï¼ˆShock Therapyï¼‰
            #   - img_weight = 0.001ï¼ˆä¿ç•™ä¸€é»é»ç´„æŸï¼Œé¿å…æ•¸å€¼æº¢å‡ºï¼‰
            #   - wm_weight = 10.0ï¼ˆå¼·è¿«æ¨¡å‹å„ªå…ˆå»ºç«‹é€šè¨Šï¼ŒBER å…ˆé™ä¸‹ä¾†ï¼‰
            #   - gan_weight = 0.0 (å®Œå…¨ç¦ç”¨ GAN)
            #   - é æœŸ PSNR æœƒå¤§å¹…ä¸‹é™ï¼ˆå¯æ¥å—ï¼‰ï¼Œå¾ŒçºŒ GAN Phase ä¿®å›ç•«è³ª
            #
            # Phase 2 (Epoch 11+): åŠ å…¥ GANï¼Œç¶­æŒæµ®æ°´å°å„ªå…ˆ
            #   - img_weight = 1.0, wm_weight = 2.0
            #   - gan_weight = 0.001 (å•Ÿç”¨ GAN)
            # ============================================================
            if gan_enabled:
                # Phase 2: ç¶­æŒæµ®æ°´å°å„ªå…ˆç´š
                current_img_weight = 1.0
                current_wm_weight = 2.0
                current_gan_weight = 0.001
            else:
                # Phase 1: Warm-up â€” å„ªå…ˆå»ºç«‹é€šè¨Šï¼Œimg_weight=0.001 ä¿ç•™ä¸€é»é»ç´„æŸé¿å…æ•¸å€¼æº¢å‡º
                current_img_weight = 0.001
                current_wm_weight = 10.0   # åŸ 20 æ˜“å°è‡´éå¼·æ®˜å·®ï¼Œé‡æ”»æ“Š BER æ˜“å´©ï¼›ç•¥é™ä»¥åˆ©æ¼¸é€²æŠ—æ”»æ“Š
                current_gan_weight = 0.0
            
            g_loss = current_img_weight * img_loss + current_wm_weight * wm_loss + current_gan_weight * g_gan_loss
            g_loss.backward()
            # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)
            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)
            opt_gen.step()
            
            # çµ±è¨ˆ
            with torch.no_grad():
                ber = (extracted.round() != watermarks).float().mean().item()
                psnr = 10 * torch.log10(1.0 / mse_img_loss.clamp(min=1e-8)).item()
                
            train_losses['g_loss'] += g_loss.item()
            train_losses['d_loss'] += d_loss.item()
            train_losses['ber'] += ber
            train_losses['psnr'] += psnr
            num_batches += 1
            
            if batch_idx % 50 == 0:
                # é¡¯ç¤ºç•¶å‰è¨“ç·´éšæ®µ
                if gan_enabled:
                    phase_str = "Phase3"
                elif noise_layer.enable_attacks:
                    phase_str = "Phase2"
                else:
                    phase_str = "Phase1"
                print(f"[{phase_str}] Epoch [{epoch}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] "
                      f"G_loss: {g_loss.item():.4f}, D_loss: {d_loss.item():.4f}, "
                      f"BER: {ber:.4f}, PSNR: {psnr:.2f}dB")
        
        # å¹³å‡è¨“ç·´æå¤±ï¼ˆé¿å…ç©º DataLoader é™¤é›¶ï¼‰
        if num_batches > 0:
            for key in train_losses:
                train_losses[key] /= num_batches
        
        # è¨ˆç®—è¨“ç·´æ™‚é•·
        train_duration = time.time() - epoch_start_time
        
        # å¯«å…¥è¨“ç·´ CSV
        write_losses_to_csv(train_csv_path, train_losses, epoch + 1, train_duration)
        
        # ============= é©—è­‰éšæ®µ =============
        encoder.eval()
        decoder.eval()
        discriminator.eval()
        
        val_losses = {'ber': 0, 'ber_clean': 0, 'psnr': 0, 'ssim': 0}
        num_val_batches = 0
        
        with torch.no_grad():
            # é©—è­‰æ™‚ä¹Ÿä½¿ç”¨ç›¸åŒçš„ Warm-up è¨­ç½®
            noise_layer.set_epoch(epoch)
            for images, watermarks in val_loader:
                images, watermarks = images.to(device), watermarks.to(device)
                
                watermarked = encoder(images, watermarks)
                noised = noise_layer(watermarked, original_image=images)
                extracted, _ = decoder(noised)
                extracted_clean, _ = decoder(watermarked)  # ç„¡æ”»æ“Š BERï¼ˆè¨ºæ–·ï¼šä¹¾æ·¨åœ–è§£ç¢¼èƒ½åŠ›ï¼‰
                
                ber = (extracted.round() != watermarks).float().mean().item()
                ber_clean = (extracted_clean.round() != watermarks).float().mean().item()
                mse = mse_loss(watermarked, images).item()
                psnr = 10 * np.log10(1.0 / max(mse, 1e-8))
                ssim_val = 1 - ssim_loss(watermarked, images).item()
                
                val_losses['ber'] += ber
                val_losses['ber_clean'] += ber_clean
                val_losses['psnr'] += psnr
                val_losses['ssim'] += ssim_val
                num_val_batches += 1
        
        if num_val_batches > 0:
            for key in val_losses:
                val_losses[key] /= num_val_batches
        
        # è¨ˆç®—ç¸½æ™‚é•·ï¼ˆè¨“ç·´+é©—è­‰ï¼‰
        total_duration = time.time() - epoch_start_time
        
        # å¯«å…¥é©—è­‰ CSV
        write_losses_to_csv(validation_csv_path, val_losses, epoch + 1, total_duration)
        
        print(f"\n{'='*80}")
        print(f"Epoch {epoch + 1}/{epochs} å®Œæˆ")
        print(f"è¨“ç·´ - G_loss: {train_losses['g_loss']:.4f}, BER: {train_losses['ber']:.4f}, PSNR: {train_losses['psnr']:.2f}dB")
        print(f"é©—è­‰ - BER(å«æ”»æ“Š): {val_losses['ber']:.4f}, BER(ç„¡æ”»æ“Š): {val_losses['ber_clean']:.4f}, PSNR: {val_losses['psnr']:.2f}dB, SSIM: {val_losses['ssim']:.4f}")
        print(f"{'='*80}\n")
        
        # å­¸ç¿’ç‡èª¿æ•´
        scheduler_gen.step()
        scheduler_disc.step()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ + æ—©åœæ©Ÿåˆ¶
        if val_losses['ber'] < best_val_ber:
            best_val_ber = val_losses['ber']
            patience_counter = 0  # é‡ç½®è€å¿ƒè¨ˆæ•¸å™¨
            torch.save({
                'epoch': epoch,
                'encoder_state_dict': encoder.state_dict(),
                'decoder_state_dict': decoder.state_dict(),
                'discriminator_state_dict': discriminator.state_dict(),
                'opt_gen_state_dict': opt_gen.state_dict(),
                'opt_disc_state_dict': opt_disc.state_dict(),
                'best_val_ber': best_val_ber,
                'val_psnr': val_losses['psnr'],
                'val_ssim': val_losses['ssim'],
            }, save_dir / 'best_model.pth')
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (BER: {best_val_ber:.4f})")
        else:
            patience_counter += 1
            print(f"â³ é©—è­‰ BER æœªæ”¹å–„ ({patience_counter}/{early_stopping_patience})")
        
        # æ¯å€‹ epoch éƒ½ä¿å­˜ checkpoint
        torch.save({
            'epoch': epoch,
            'encoder_state_dict': encoder.state_dict(),
            'decoder_state_dict': decoder.state_dict(),
            'discriminator_state_dict': discriminator.state_dict(),
            'opt_gen_state_dict': opt_gen.state_dict(),
            'opt_disc_state_dict': opt_disc.state_dict(),
            'scheduler_gen_state_dict': scheduler_gen.state_dict(),
            'scheduler_disc_state_dict': scheduler_disc.state_dict(),
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_val_ber': best_val_ber,
        }, save_dir / f'checkpoint_epoch_{epoch}.pth')
        print(f"âœ“ ä¿å­˜æª¢æŸ¥é»: checkpoint_epoch_{epoch}.pth")
        
        # æ—©åœæª¢æŸ¥ï¼ˆåªåœ¨ Warm-up çµæŸå¾Œå•Ÿç”¨ï¼‰
        if epoch >= GAN_WARMUP_EPOCHS and patience_counter >= early_stopping_patience:
            print(f"\n{'='*60}")
            print(f"ğŸ›‘ æ—©åœè§¸ç™¼ï¼šé©—è­‰ BER åœ¨ {early_stopping_patience} epochs å…§æœªæ”¹å–„")
            print(f"   æœ€ä½³ BER: {best_val_ber:.4f}")
            print(f"{'='*60}\n")
            break
    
    print("\nè¨“ç·´å®Œæˆï¼")
    return encoder, decoder, discriminator

# Test Function (æ”¹é€²ç‰ˆï¼šæ›´è©³ç´°çš„è©•ä¼°)
def test_model(checkpoint_path, image_path, watermark_bits=64, device='cuda', save_dir='./test_results'):
    save_dir = Path(save_dir)
    save_dir.mkdir(exist_ok=True, parents=True)
    
    # è¼‰å…¥æ¨¡å‹
    checkpoint = torch.load(checkpoint_path, map_location=device)
    encoder = Encoder(watermark_bits).to(device)
    decoder = Decoder(watermark_bits).to(device)
    encoder.load_state_dict(checkpoint['encoder_state_dict'])
    decoder.load_state_dict(checkpoint['decoder_state_dict'])
    encoder.eval()
    decoder.eval()
    
    noise_layer = NoiseLayer(device).to(device)
    # æ¸¬è©¦æ”»æ“Šæ™‚å¿…é ˆå•Ÿç”¨ Noise Layerï¼ˆå¦å‰‡ enable_attacks=False æœƒç›´æ¥å›å‚³åŸåœ–ï¼‰
    noise_layer.set_epoch(10)
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])
    image = transform(Image.open(image_path).convert('RGB')).unsqueeze(0).to(device)
    watermark = torch.randint(0, 2, (1, watermark_bits)).float().to(device)
    
    print(f"\n{'='*80}")
    print(f"æ¸¬è©¦åœ–åƒ: {image_path}")
    print(f"æ°´å°ä½æ•¸: {watermark_bits}")
    print(f"{'='*80}\n")
    
    with torch.no_grad():
        # åµŒå…¥æ°´å°
        watermarked = encoder(image, watermark)
        
        # è¨ˆç®—åœ–åƒå“è³ªæŒ‡æ¨™
        mse = F.mse_loss(watermarked, image).item()
        psnr = 10 * np.log10(1.0 / max(mse, 1e-8))
        ssim_val = 1 - ssim_loss(watermarked, image).item()
        
        print(f"åŸå§‹åµŒå…¥å“è³ª:")
        print(f"  PSNR: {psnr:.2f} dB")
        print(f"  SSIM: {ssim_val:.4f}")
        print(f"  MSE:  {mse:.6f}\n")
        
        # æ¸¬è©¦ä¸åŒæ”»æ“Šä¸‹çš„ BER
        attacks = ['gaussian', 'jpeg', 'crop', 'dropout', 'resize']
        print(f"æ”»æ“Šé­¯æ£’æ€§æ¸¬è©¦:")
        print(f"{'-'*80}")
        
        for attack in attacks:
            noise_layer.attacks = [attack]
            noised = noise_layer(watermarked, original_image=image)
            extracted, _ = decoder(noised)
            ber = (extracted.round() != watermark).float().mean().item()
            print(f"  {attack:15s}: BER = {ber:.4f} ({int(ber * watermark_bits)}/{watermark_bits} bits)")
        
        # ç„¡æ”»æ“Šçš„ BER
        extracted_clean, _ = decoder(watermarked)
        ber_clean = (extracted_clean.round() != watermark).float().mean().item()
        print(f"  {'no_attack':15s}: BER = {ber_clean:.4f} ({int(ber_clean * watermark_bits)}/{watermark_bits} bits)")
        print(f"{'-'*80}\n")
        
        # ä¿å­˜åœ–åƒ
        transforms.ToPILImage()(watermarked[0].cpu()).save(save_dir / 'watermarked.png')
        transforms.ToPILImage()(image[0].cpu()).save(save_dir / 'original.png')
        print(f"âœ“ çµæœå·²ä¿å­˜è‡³ {save_dir}")
        
        # è¦–è¦ºåŒ–æ°´å°å°æ¯”
        diff = torch.abs(watermarked - image) * 10  # æ”¾å¤§å·®ç•°ä»¥ä¾¿è§€å¯Ÿ
        transforms.ToPILImage()(diff[0].cpu()).save(save_dir / 'difference_x10.png')
        
    return {
        'psnr': psnr,
        'ssim': ssim_val,
        'ber_clean': ber_clean,
    }

# Main (æ”¹é€²ç‰ˆ)
if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='æ”¹é€²ç‰ˆ ARWGAN æ°´å°æ¨¡å‹')
    parser.add_argument('--train', action='store_true', help='è¨“ç·´æ¨¡å¼')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼')
    parser.add_argument('--image', type=str, default='test.jpg', help='æ¸¬è©¦åœ–åƒè·¯å¾‘')
    parser.add_argument('--checkpoint', type=str, default='./checkpoints_improved/best_model.pth', help='checkpoint è·¯å¾‘ï¼ˆæ¸¬è©¦ç”¨ï¼‰')
    parser.add_argument('--resume', type=str, default=None, help='å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´ï¼ˆè¨“ç·´ç”¨ï¼‰')
    parser.add_argument('--epochs', type=int, default=100, help='è¨“ç·´ epochs')
    parser.add_argument('--batch', type=int, default=16, help='Batch size')
    parser.add_argument('--lr', type=float, default=None, help='å­¸ç¿’ç‡ï¼ˆå·²æ£„ç”¨ï¼šä½¿ç”¨å·®åˆ†å­¸ç¿’ç‡ï¼ŒEncoder=1e-4, Decoder=1e-3ï¼‰')
    parser.add_argument('--use_vgg', action='store_true', help='ä½¿ç”¨ VGG æ„ŸçŸ¥æå¤±')
    parser.add_argument('--save_dir', type=str, default='./checkpoints_improved', help='æ¨¡å‹ä¿å­˜ç›®éŒ„')
    parser.add_argument('--data-dir', type=str, default=None, help='æ•¸æ“šé›†ç›®éŒ„è·¯å¾‘ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œæœƒè‡ªå‹•æª¢æ¸¬ï¼‰')
    args = parser.parse_args()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    if args.train:
        print("\né–‹å§‹è¨“ç·´æ”¹é€²ç‰ˆ ARWGAN æ¨¡å‹...")
        train_model(
            epochs=args.epochs, 
            batch_size=args.batch, 
            lr=None,  # ä½¿ç”¨å›ºå®šçš„å·®åˆ†å­¸ç¿’ç‡ï¼Œå¿½ç•¥å‘½ä»¤è¡Œåƒæ•¸
            device=device,
            save_dir=args.save_dir,
            use_vgg=args.use_vgg,
            resume_from_checkpoint=args.resume,
            data_dir=args.data_dir
        )
    
    if args.test:
        print("\né–‹å§‹æ¸¬è©¦æ¨¡å‹...")
        if not Path(args.checkpoint).exists():
            print(f"éŒ¯èª¤: checkpoint ä¸å­˜åœ¨: {args.checkpoint}")
        else:
            test_model(
                checkpoint_path=args.checkpoint,
                image_path=args.image,
                device=device
            )

# ------------------- ä¿®å¾©èˆ‡æ”¹é€²èªªæ˜ -------------------
# ã€å·²ä¿®å¾©çš„å•é¡Œã€‘
# 1. SpatialAttention Bug: ä¿®æ­£ç‚ºç”¨ attention mask ä¹˜ä»¥åŸå§‹è¼¸å…¥ï¼ˆline 57-63ï¼‰
# 2. Encoder è¼¸å‡º: æ”¹ç”¨ 1x1 conv æ˜ å°„ 64â†’3 channelsï¼Œä¿ç•™æ›´å¤šè³‡è¨Šï¼ˆline 95, 123ï¼‰
# 3. JPEG æ¨¡æ“¬: ä½¿ç”¨é«˜æ–¯å™ªè²æ¨¡æ“¬ JPEG å£“ç¸®æ•ˆæœï¼ˆå¯å¾®åˆ†ï¼‰
# 4. NoiseLayer å®‰å…¨æ€§: ä¿®å¾©ç´¢å¼•è¶Šç•Œå•é¡Œï¼ŒåŠ å…¥é‚Šç•Œæª¢æŸ¥ï¼ˆline 308-316ï¼‰
#
# ã€æ–°å¢åŠŸèƒ½ã€‘
# 1. VGG æ„ŸçŸ¥æå¤±: æå‡è¦–è¦ºå“è³ªï¼ˆline 264-273ï¼‰
# 2. è¨“ç·´/é©—è­‰é›†åˆ†é›¢: 90/10 splitï¼Œé¿å…éæ“¬åˆï¼ˆline 322-326ï¼‰
# 3. å­¸ç¿’ç‡èª¿åº¦: StepLRï¼Œæ¯ 30 epochs è¡°æ¸› 0.5ï¼ˆline 342-343ï¼‰
# 4. Checkpoint ç³»çµ±: è‡ªå‹•ä¿å­˜æœ€ä½³æ¨¡å‹å’Œå®šæœŸæª¢æŸ¥é»ï¼ˆline 403-417ï¼‰
# 5. è©³ç´°è©•ä¼°: å¤šæ”»æ“Šæ¸¬è©¦ã€PSNR/SSIM/BER å…¨é¢æŒ‡æ¨™ï¼ˆline 432-479ï¼‰
#
# ã€æ¶æ§‹å„ªå‹¢ã€‘
# 1. CBAM Attention: Channel + Spatial é›™é‡æ³¨æ„åŠ›ï¼Œå„ªæ–¼ softmax attention
# 2. ResNet-Style Decoder: ç´”ä¸‹æ¡æ¨£ CNNï¼Œå°ˆç‚ºåˆ†é¡ä»»å‹™è¨­è¨ˆ
# 3. WGAN-GP: ç©©å®š GAN è¨“ç·´ï¼Œé¿å… mode collapse
# 4. Dense Connections: ä¿ç•™å¤šå±¤ç‰¹å¾µï¼Œå¢å¼·è¡¨é”èƒ½åŠ›
#
# ã€é æœŸæ€§èƒ½ã€‘
# - PSNR: >30 dB (å„ªæ–¼åŸè«–æ–‡çš„ 28dB)
# - BER: <0.02 under mixed attacks
# - SSIM: >0.95
# - è¨“ç·´æ™‚é–“: RTX 3090 ç´„ 6-8 å°æ™‚ (100 epochs)
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
# è¨“ç·´: python watermark_model_better.py --train --epochs 100 --batch 16 --use_vgg
# æ¸¬è©¦: python watermark_model_better.py --test --checkpoint ./checkpoints_improved/best_model.pth --image test.jpg
#
# ã€èˆ‡åŸ ARWGAN å°æ¯”ã€‘
# å„ªå‹¢: CBAM attentionã€WGAN-GPã€VGG lossã€æ›´å®Œæ•´çš„è¨“ç·´æ¡†æ¶
# ç›¸å®¹æ€§: å¯ç›´æ¥æ›¿æ›åŸæ¨¡å‹ï¼Œä½¿ç”¨ç›¸åŒæ•¸æ“šé›†
# ------------------------------------------------------------