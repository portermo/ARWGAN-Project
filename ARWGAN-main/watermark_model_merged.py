"""
ARWGAN Merged Model - åˆä½µç‰ˆ
============================
æ­¤æª”æ¡ˆå°‡å·²é©—è­‰å¯æ”¶æ–‚çš„ ARWGAN åŸå§‹æ¨¡å‹æ¶æ§‹ï¼Œ
ç§»æ¤è‡³æ”¹é€²çš„è¨“ç·´æ¡†æ¶ï¼ˆæ¼¸é€²å¼ Noise Layerã€Warm-upã€Checkpointingï¼‰ã€‚

ã€æ¶æ§‹ä¾†æºã€‘
- Encoder/Decoder/Discriminator: model/encoder.py, model/decoder.py, model/discriminator.py
- Dense Block (Bottleneck): model/Dense_block.py

ã€è¨“ç·´æ¡†æ¶ä¾†æºã€‘
- NoiseLayer (æ¼¸é€²æ”»æ“Š): watermark_model_better.py
- train_model (Warm-up, Checkpoint, CSV Logging): watermark_model_better.py

ã€é‡è¦é©é…ã€‘
- åŸå§‹ ARWGAN ä½¿ç”¨åœ–åƒç¯„åœ [-1, 1]ï¼ˆPSNR è¨ˆç®— MAX^2=4ï¼‰
- æœ¬æª”æ¡ˆé©é…ç‚º [0, 1] ç¯„åœä»¥ç›¸å®¹ç¾æœ‰ DataLoader å’Œ Loss
- Encoder è¼¸å‡ºåŠ å…¥ clamp(0, 1)

é‹è¡Œ: python watermark_model_merged.py --train --epochs 100 --batch 16 --use_vgg --data-dir data/coco2017
ï¼ˆå·²å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´ AMPï¼Œbatch 16 æ‡‰å¯åœ¨ 24GB GPU é‹è¡Œï¼›è‹¥ä» OOMï¼Œè«‹æ”¹ç”¨ --batch 8ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
import torchvision.transforms as transforms
import torchvision
import numpy as np
import random
import os
import csv
import time
import multiprocessing
from pathlib import Path
from PIL import Image


# ============================================================
# Dense Block (Bottleneck) - ä¾†è‡ª model/Dense_block.py
# ============================================================
class Bottleneck(nn.Module):
    """
    DenseNet é¢¨æ ¼çš„ Bottleneck æ¨¡çµ„
    - 1x1 Conv é™ç¶­ â†’ 3x3 Conv ç‰¹å¾µæå–
    - æ”¯æ´ Dense Connection (last=False) æˆ–å–®ç¨è¼¸å‡º (last=True)
    """
    def __init__(self, nChannels, growthRate):
        super(Bottleneck, self).__init__()
        interChannels = 4 * growthRate
        self.relu = nn.ReLU(inplace=True)
        self.bn1 = nn.BatchNorm2d(nChannels)
        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(interChannels)
        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3, padding=1, bias=False)

    def forward(self, x, last=False):
        out = self.conv1(self.relu(self.bn1(x)))
        out = self.conv2(self.relu(self.bn2(out)))
        if last:
            return out
        else:
            return torch.cat((x, out), 1)


# ============================================================
# Encoder - ä¾†è‡ª model/encoder.py (å·²é©é…)
# ============================================================
class Encoder(nn.Module):
    """
    ARWGAN åŸå§‹ Encoder æ¶æ§‹
    
    ç‰¹é»ï¼š
    - Dense Connection ä¿ç•™å¤šå±¤ç‰¹å¾µ
    - Attention Mask (Softmax) å¼•å°åµŒå…¥ä½ç½®
    - æ®˜å·®é€£æ¥ (im_w + image)
    
    é©é…ï¼š
    - ç§»é™¤ HiDDenConfiguration ä¾è³´
    - è¼¸å‡º clamp åˆ° [0, 1] ä»¥ç›¸å®¹ç¾æœ‰è¨“ç·´æµç¨‹
      ï¼ˆåŸå§‹ ARWGAN ä½¿ç”¨ [-1, 1] ç¯„åœï¼‰
    """
    
    def conv2(self, in_channel, out_channel):
        return nn.Conv2d(in_channels=in_channel,
                         out_channels=out_channel,
                         stride=1,
                         kernel_size=3,
                         padding=1)

    def __init__(self, watermark_bits=64, channels=64):
        super(Encoder, self).__init__()
        self.watermark_bits = watermark_bits
        self.conv_channels = channels
        
        # ç¬¬ä¸€å±¤ï¼šæå–åˆå§‹ç‰¹å¾µ
        self.first_layer = nn.Sequential(
            self.conv2(3, self.conv_channels)
        )

        self.second_layer = nn.Sequential(
            self.conv2(self.conv_channels, self.conv_channels),
            nn.BatchNorm2d(self.conv_channels),
            nn.LeakyReLU(inplace=True),
        )

        self.third_layer = nn.Sequential(
            self.conv2(self.conv_channels * 2, self.conv_channels),
            nn.BatchNorm2d(self.conv_channels),
            nn.LeakyReLU(inplace=True),
            self.conv2(self.conv_channels, self.conv_channels),
            nn.BatchNorm2d(self.conv_channels),
            nn.LeakyReLU(inplace=True),
        )

        self.fourth_layer = nn.Sequential(
            self.conv2(self.conv_channels * 3 + watermark_bits, self.conv_channels),
            nn.BatchNorm2d(self.conv_channels),
            nn.LeakyReLU(inplace=True)
        )

        # Dense Blocksï¼ˆèåˆ watermarkï¼‰
        self.Dense_block1 = Bottleneck(self.conv_channels + watermark_bits, self.conv_channels)
        self.Dense_block2 = Bottleneck(self.conv_channels * 2 + watermark_bits, self.conv_channels)
        self.Dense_block3 = Bottleneck(self.conv_channels * 3 + watermark_bits, self.conv_channels)
        
        # Dense Blocksï¼ˆAttention åˆ†æ”¯ï¼‰
        self.Dense_block_a1 = Bottleneck(self.conv_channels, self.conv_channels)
        self.Dense_block_a2 = Bottleneck(self.conv_channels * 2, self.conv_channels)
        self.Dense_block_a3 = Bottleneck(self.conv_channels * 3, self.conv_channels)

        # ç¬¬äº”å±¤ï¼šç”Ÿæˆ watermark ç‰¹å¾µ
        self.fifth_layer = nn.Sequential(
            nn.BatchNorm2d(self.conv_channels + watermark_bits),
            nn.ReLU(inplace=True),
            self.conv2(self.conv_channels + watermark_bits, self.conv_channels),
            nn.BatchNorm2d(self.conv_channels),
            nn.ReLU(inplace=True),
            self.conv2(self.conv_channels, watermark_bits),
        )
        
        # ç¬¬å…­å±¤ï¼šAttention Mask ç”Ÿæˆ
        self.sixth_layer = nn.Sequential(
            nn.BatchNorm2d(self.conv_channels),
            nn.ReLU(inplace=True),
            self.conv2(self.conv_channels, self.conv_channels),
            nn.BatchNorm2d(self.conv_channels),
            nn.ReLU(inplace=True),
            self.conv2(self.conv_channels, watermark_bits),
            nn.Softmax(dim=1)  # Attention mask: å­¸ç¿’åµŒå…¥ä½ç½®
        )

        # æœ€çµ‚å±¤ï¼šå¾ watermark ç‰¹å¾µç”Ÿæˆ RGB æ®˜å·®
        self.final_layer = nn.Sequential(
            nn.Conv2d(watermark_bits, 3, kernel_size=3, padding=1),
        )

    def forward(self, image, message):
        """
        Args:
            image: (B, 3, H, W) ç¯„åœ [0, 1]
            message: (B, watermark_bits) äºŒé€²åˆ¶æµ®æ°´å°
        Returns:
            watermarked_image: (B, 3, H, W) ç¯„åœ [0, 1]
        """
        H, W = image.size()[2], image.size()[3]

        # æ“´å±• message åˆ°ç©ºé–“ç¶­åº¦
        expanded_message = message.unsqueeze(-1).unsqueeze(-1)
        expanded_message = expanded_message.expand(-1, -1, H, W)

        # ä¸»å¹¹ï¼šDense Connection + Watermark èåˆ
        feature0 = self.first_layer(image)
        feature1 = self.Dense_block1(torch.cat((feature0, expanded_message), 1), last=True)
        feature2 = self.Dense_block2(torch.cat((feature0, expanded_message, feature1), 1), last=True)
        feature3 = self.Dense_block3(torch.cat((feature0, expanded_message, feature1, feature2), 1), last=True)
        feature3 = self.fifth_layer(torch.cat((feature3, expanded_message), 1))
        
        # Attention åˆ†æ”¯ï¼šå­¸ç¿’åµŒå…¥ä½ç½®
        feature_attention = self.Dense_block_a3(
            self.Dense_block_a2(
                self.Dense_block_a1(feature0)
            ), 
            last=True
        )
        # Attention mask * 30 æ”¾å¤§ï¼ˆåŸå§‹ ARWGAN è¨­è¨ˆï¼‰
        feature_mask = self.sixth_layer(feature_attention) * 30
        
        # ç‰¹å¾µ Ã— Attention Mask
        feature = feature3 * feature_mask
        
        # ç”Ÿæˆ RGB æ®˜å·®ä¸¦åŠ åˆ°åŸåœ–
        im_w = self.final_layer(feature)
        im_w = im_w + image
        
        # ============================================================
        # ã€é©é…ã€‘Clamp åˆ° [0, 1]
        # åŸå§‹ ARWGAN ä½¿ç”¨ [-1, 1] ç¯„åœï¼ˆç„¡ clampï¼‰
        # ç‚ºäº†ç›¸å®¹ç¾æœ‰è¨“ç·´æ¡†æ¶ï¼Œåœ¨æ­¤åŠ å…¥ clamp
        # ============================================================
        clamped = torch.clamp(im_w, 0, 1)
        return clamped


# ============================================================
# Decoder - ä¾†è‡ª model/decoder.py (å·²é©é…)
# ============================================================
class Decoder(nn.Module):
    """
    ARWGAN åŸå§‹ Decoder æ¶æ§‹
    
    ç‰¹é»ï¼š
    - Dense Connection ä¿ç•™å¤šå±¤ç‰¹å¾µ
    - AdaptiveAvgPool + Linear è¼¸å‡º logits
    
    è¼¸å‡ºï¼š
    - extracted: äºŒå€¼åŒ–å¾Œçš„ watermark (B, watermark_bits)
    - logits: åŸå§‹ logitsï¼Œç”¨æ–¼ BCE Loss (B, watermark_bits)
    """
    
    def conv2(self, in_channel, out_channel):
        return nn.Conv2d(in_channels=in_channel,
                         out_channels=out_channel,
                         stride=1,
                         kernel_size=3,
                         padding=1)

    def __init__(self, watermark_bits=64, channels=64):
        super(Decoder, self).__init__()
        self.watermark_bits = watermark_bits
        self.channels = channels

        self.first_layer = nn.Sequential(
            self.conv2(3, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        self.second_layer = nn.Sequential(
            self.conv2(self.channels, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        self.third_layer = nn.Sequential(
            self.conv2(self.channels * 2, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        self.fourth_layer = nn.Sequential(
            self.conv2(self.channels * 3, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        # Dense Blocks
        self.Dense_block1 = Bottleneck(self.channels, self.channels)
        self.Dense_block2 = Bottleneck(self.channels * 2, self.channels)
        self.Dense_block3 = Bottleneck(self.channels * 3, self.channels)

        self.fifth_layer = nn.Sequential(
            self.conv2(self.channels, watermark_bits),
            nn.BatchNorm2d(watermark_bits),
            nn.ReLU(inplace=True)
        )

        self.pooling = nn.AdaptiveAvgPool2d(output_size=(1, 1))
        self.linear = nn.Linear(watermark_bits, watermark_bits)

    def forward(self, image_with_wm):
        """
        Args:
            image_with_wm: (B, 3, H, W) å«æµ®æ°´å°çš„åœ–åƒ
        Returns:
            extracted: (B, watermark_bits) äºŒå€¼åŒ–å¾Œçš„ watermark
            logits: (B, watermark_bits) åŸå§‹ logitsï¼ˆç”¨æ–¼ BCE Lossï¼‰
        """
        feature0 = self.first_layer(image_with_wm)
        feature1 = self.second_layer(feature0)
        feature2 = self.third_layer(torch.cat([feature0, feature1], dim=1))
        feature3 = self.fourth_layer(torch.cat([feature0, feature1, feature2], dim=1))
        x = self.fifth_layer(feature3)
        x = self.pooling(x)
        logits = self.linear(x.squeeze(3).squeeze(2))
        
        # äºŒå€¼åŒ–è¼¸å‡ºï¼ˆç”¨æ–¼è¨ˆç®— BERï¼‰
        extracted = (torch.sigmoid(logits) > 0.5).float()
        
        return extracted, logits


# ============================================================
# Discriminator - ä¾†è‡ª model/discriminator.py (å·²é©é…)
# ============================================================
class Discriminator(nn.Module):
    """
    ARWGAN åŸå§‹ Discriminator æ¶æ§‹
    
    ç‰¹é»ï¼š
    - Dense Connection
    - AdaptiveAvgPool + Linear è¼¸å‡ºå–®ä¸€ scalar
    - é©ç”¨æ–¼ WGAN-GP æˆ–æ¨™æº– GAN
    """
    
    def conv2(self, in_channel, out_channel):
        return nn.Conv2d(in_channels=in_channel,
                         out_channels=out_channel,
                         stride=1,
                         kernel_size=3,
                         padding=1)

    def __init__(self, watermark_bits=64, channels=64):
        super(Discriminator, self).__init__()
        self.channels = channels

        self.first_layer = nn.Sequential(
            self.conv2(3, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        self.second_layer = nn.Sequential(
            self.conv2(self.channels, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        self.third_layer = nn.Sequential(
            self.conv2(self.channels * 2, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        self.fourth_layer = nn.Sequential(
            self.conv2(self.channels * 3, self.channels),
            nn.BatchNorm2d(self.channels),
            nn.LeakyReLU(inplace=True)
        )

        # Dense Blocks
        self.Dense_block1 = Bottleneck(self.channels, self.channels)
        self.Dense_block2 = Bottleneck(self.channels * 2, self.channels)
        self.Dense_block3 = Bottleneck(self.channels * 3, self.channels)
        
        self.fifth_layer = nn.Sequential(
            self.conv2(self.channels, watermark_bits),
            nn.BatchNorm2d(watermark_bits),
            nn.LeakyReLU(inplace=True)
        )

        self.average = nn.AdaptiveAvgPool2d(output_size=(1, 1))
        self.linear = nn.Linear(watermark_bits, 1)

    def forward(self, image):
        """
        Args:
            image: (B, 3, H, W)
        Returns:
            scalar: åˆ¤åˆ¥åˆ†æ•¸ï¼ˆç”¨æ–¼ GAN Lossï¼‰
        """
        feature0 = self.first_layer(image)
        feature1 = self.second_layer(feature0)
        feature2 = self.third_layer(torch.cat([feature0, feature1], dim=1))
        feature3 = self.fourth_layer(torch.cat([feature0, feature1, feature2], dim=1))
        x = self.fifth_layer(feature3)
        x = self.average(x)
        x = x.squeeze(3).squeeze(2)
        x = self.linear(x)
        result = x.mean()  # è¿”å› batch å¹³å‡ï¼ˆWGAN é¢¨æ ¼ï¼‰
        return result


# ============================================================
# ä»¥ä¸‹ç‚º watermark_model_better.py çš„è¨“ç·´æ¡†æ¶ï¼ˆä¿ç•™ï¼‰
# ============================================================

# JPEG å™ªè²æ¨¡æ“¬å™¨
class JPEGNoiseSimulator(nn.Module):
    """
    ç°¡åŒ–çš„ JPEG å£“ç¸®æ¨¡æ“¬å™¨ï¼ˆé«˜æ–¯å™ªè²æ¨¡æ“¬ï¼‰
    """
    def __init__(self, device):
        super(JPEGNoiseSimulator, self).__init__()
        self.device = device
    
    def forward(self, x, quality_factor=50):
        quality_scale = (100 - quality_factor) / 100.0
        noise_std = 0.02 + quality_scale * 0.08
        noised = x + torch.randn_like(x) * noise_std
        return torch.clamp(noised, 0, 1)


# Noise Layerï¼ˆæ¼¸é€²å¼æ”»æ“Šï¼‰
class NoiseLayer(nn.Module):
    """
    æ¼¸é€²å¼ Noise Layer
    - warmup_epochs: å‰ N å€‹ epoch é—œé–‰æ”»æ“Š
    - noise_ramp_epochs: æ”»æ“Šå¼·åº¦ç·šæ€§å¢é•·
    """
    def __init__(self, device, attacks=['gaussian', 'jpeg', 'crop', 'dropout', 'resize'], warmup_epochs=5):
        super(NoiseLayer, self).__init__()
        self.attacks = attacks
        self.device = device
        self.jpeg_simulator = JPEGNoiseSimulator(device)
        self.warmup_epochs = warmup_epochs
        self.noise_ramp_epochs = 10
        self.current_epoch = 0
        self.enable_attacks = False
        self.attack_prob = 0.0

    def gaussian_noise(self, x, std=0.05):
        noise = torch.randn_like(x) * std
        return torch.clamp(x + noise, 0, 1)

    def jpeg_compression(self, x, quality=50):
        return self.jpeg_simulator(x, quality_factor=quality)

    def crop(self, x, ratio=0.1):
        B, C, H, W = x.shape
        crop_h = max(1, int(H * ratio))
        crop_w = max(1, int(W * ratio))
        start_h = random.randint(0, max(0, H - crop_h))
        start_w = random.randint(0, max(0, W - crop_w))
        cropped = x[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w]
        pad_left, pad_right = start_w, W - start_w - crop_w
        pad_top, pad_bottom = start_h, H - start_h - crop_h
        padded = F.pad(cropped, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0)
        assert padded.shape == (B, C, H, W), f"crop pad shape mismatch: got {padded.shape}, expected ({B}, {C}, {H}, {W})"
        return padded

    def dropout(self, x, original_image, ratio=0.1):
        B, C, H, W = x.shape
        block_h = max(1, int(H * ratio))
        block_w = max(1, int(W * ratio))
        start_h = random.randint(0, max(1, H - block_h))
        start_w = random.randint(0, max(1, W - block_w))
        x_clone = x.clone()
        x_clone[:, :, start_h:start_h+block_h, start_w:start_w+block_w] = original_image[:, :, start_h:start_h+block_h, start_w:start_w+block_w]
        return x_clone

    def resize(self, x, scale=0.5):
        return F.interpolate(F.interpolate(x, scale_factor=scale, mode='bicubic', align_corners=False), 
                           size=x.shape[2:], mode='bicubic', align_corners=False)

    def set_epoch(self, epoch):
        self.current_epoch = epoch
        self.enable_attacks = (epoch >= self.warmup_epochs)
        if epoch < self.warmup_epochs:
            self.attack_prob = 0.0
        else:
            ramp = (epoch - self.warmup_epochs) / max(1, self.noise_ramp_epochs)
            self.attack_prob = min(1.0, ramp)
    
    def forward(self, x, original_image=None):
        if not self.enable_attacks:
            return x
        if random.random() >= self.attack_prob:
            return x
        attack = random.choice(self.attacks)
        if attack == 'gaussian':
            return self.gaussian_noise(x)
        elif attack == 'jpeg':
            return self.jpeg_compression(x)
        elif attack == 'crop':
            return self.crop(x)
        elif attack == 'dropout' and original_image is not None:
            return self.dropout(x, original_image)
        elif attack == 'resize':
            return self.resize(x)
        return x


# VGG æ„ŸçŸ¥æå¤±
class VGGLoss(nn.Module):
    def __init__(self):
        super(VGGLoss, self).__init__()
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
        
        vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)
        # ç›´æ¥ä½¿ç”¨ features çš„å‰ 16 å€‹å±¤ï¼ˆé¿å… children() å¯èƒ½çš„å•é¡Œï¼‰
        # VGG16 features çµæ§‹ï¼šConv2d -> ReLU -> Conv2d -> ReLU -> MaxPool2d -> ... (å…± 30 å±¤)
        # å‰ 16 å±¤å°æ‡‰åˆ°ç¬¬ 3 å€‹ block çš„ ReLUï¼ˆfeature map size: 64x64ï¼‰
        self.vgg_layers = nn.Sequential(*list(vgg16.features)[:16])
        for param in self.vgg_layers.parameters():
            param.requires_grad = False

    def forward(self, x):
        x_norm = (x - self.mean) / self.std
        return self.vgg_layers(x_norm)


# SSIM Loss
def ssim_loss(img1, img2):
    mu1 = F.avg_pool2d(img1, 11, stride=1, padding=5)
    mu2 = F.avg_pool2d(img2, 11, stride=1, padding=5)
    sigma1_sq = F.avg_pool2d(img1**2, 11, stride=1, padding=5) - mu1**2
    sigma2_sq = F.avg_pool2d(img2**2, 11, stride=1, padding=5) - mu2**2
    sigma12 = F.avg_pool2d(img1*img2, 11, stride=1, padding=5) - mu1*mu2
    C1, C2 = 0.01**2, 0.03**2
    sigma1_sq = sigma1_sq.clamp(min=0)
    sigma2_sq = sigma2_sq.clamp(min=0)
    ssim = ((2*mu1*mu2 + C1) * (2*sigma12 + C2)) / ((mu1**2 + mu2**2 + C1) * (sigma1_sq + sigma2_sq + C2) + 1e-8)
    return 1 - ssim.mean()


# WGAN-GP Loss
def wgan_gp_loss(discriminator, real_imgs, fake_imgs, lambda_gp=10):
    """
    è¨ˆç®— WGAN-GP æ¢¯åº¦æ‡²ç½°
    æ³¨æ„ï¼šç¢ºä¿æ‰€æœ‰ tensor åœ¨åŒä¸€è¨­å‚™ä¸Šï¼Œé¿å…å¤šé€²ç¨‹ä¸‹çš„è¨˜æ†¶é«”å•é¡Œ
    """
    batch_size = real_imgs.size(0)
    device = real_imgs.device
    
    # ç¢ºä¿æ‰€æœ‰ tensor åœ¨åŒä¸€è¨­å‚™ä¸Š
    alpha = torch.rand(batch_size, 1, 1, 1, device=device, requires_grad=False)
    alpha = alpha.expand_as(real_imgs)
    
    # å‰µå»ºæ’å€¼æ¨£æœ¬
    interpolates = alpha * real_imgs + (1 - alpha) * fake_imgs
    interpolates = interpolates.requires_grad_(True)
    
    # è¨ˆç®—åˆ¤åˆ¥å™¨è¼¸å‡º
    disc_interpolates = discriminator(interpolates)
    
    # Discriminator è¿”å› scalar (0-d tensor)ï¼Œgrad_outputs ä¹Ÿå¿…é ˆæ˜¯ scalar
    # ä½¿ç”¨ torch.tensor(1.0) è€Œä¸æ˜¯ torch.ones(1) ä¾†å‰µå»º scalar
    if disc_interpolates.dim() == 0:
        # scalar tensorï¼Œgrad_outputs ä¹Ÿå¿…é ˆæ˜¯ scalar
        grad_outputs = torch.tensor(1.0, device=device, requires_grad=False)
    else:
        grad_outputs = torch.ones_like(disc_interpolates)
    
    # è¨ˆç®—æ¢¯åº¦
    gradients = torch.autograd.grad(
        outputs=disc_interpolates,
        inputs=interpolates,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]
    
    # è¨ˆç®—æ¢¯åº¦æ‡²ç½°
    gradients = gradients.view(batch_size, -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp
    
    return gradient_penalty


# CSV è¨˜éŒ„
def write_losses_to_csv(file_name, losses_dict, epoch, duration):
    file_exists = os.path.exists(file_name) and os.path.getsize(file_name) > 0
    with open(file_name, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            row_to_write = ['epoch'] + list(losses_dict.keys()) + ['duration']
            writer.writerow(row_to_write)
        row_to_write = [epoch] + ['{:.4f}'.format(v) for v in losses_dict.values()] + ['{:.0f}'.format(duration)]
        writer.writerow(row_to_write)


# Dataset
TARGET_IMAGE_SIZE = (256, 256)

class WatermarkDataset(Dataset):
    def __init__(self, root_dir='./data/coco/images/train2017', transform=None, watermark_bits=64):
        self.root_dir = root_dir
        self.transform = transform or transforms.Compose([
            transforms.Resize(TARGET_IMAGE_SIZE),
            transforms.ToTensor(),
        ])
        
        if not os.path.exists(root_dir):
            raise ValueError(f"æ•¸æ“šé›†ç›®éŒ„ä¸å­˜åœ¨: {root_dir}")
        
        if not os.path.isdir(root_dir):
            raise ValueError(f"æŒ‡å®šçš„è·¯å¾‘ä¸æ˜¯ç›®éŒ„: {root_dir}")
        
        self.image_list = []
        
        try:
            all_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            for f in all_files:
                img_path = os.path.join(root_dir, f)
                if os.path.exists(img_path):
                    self.image_list.append(f)
        except (OSError, PermissionError):
            pass
        
        if len(self.image_list) == 0:
            common_subdirs = ['train/images', 'train', 'images', 'train2017', 'val/images', 'val']
            for subdir in common_subdirs:
                search_path = os.path.join(root_dir, subdir)
                if os.path.exists(search_path) and os.path.isdir(search_path):
                    try:
                        files = [f for f in os.listdir(search_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                        for f in files:
                            img_path = os.path.join(search_path, f)
                            if os.path.exists(img_path):
                                self.image_list.append(f)
                        if len(self.image_list) > 0:
                            self.root_dir = search_path
                            print(f"åœ¨å­ç›®éŒ„ {subdir} ä¸­æ‰¾åˆ°åœ–ç‰‡æ–‡ä»¶")
                            break
                    except (OSError, PermissionError):
                        continue
        
        if len(self.image_list) == 0:
            print(f"åœ¨ {root_dir} ä¸­æœªæ‰¾åˆ°åœ–ç‰‡ï¼Œé–‹å§‹éè¿´æœç´¢...")
            for root, dirs, files in os.walk(root_dir):
                for f in files:
                    if f.lower().endswith(('.jpg', '.jpeg', '.png')):
                        img_path = os.path.join(root, f)
                        if os.path.exists(img_path):
                            rel_path = os.path.relpath(img_path, root_dir)
                            self.image_list.append(rel_path)
                if len(self.image_list) > 100:
                    break
        
        if len(self.image_list) == 0:
            raise ValueError(f"åœ¨ {root_dir} ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ï¼")
        
        self.image_list = [f for f in self.image_list if f and isinstance(f, str) and len(f) > 0]
        
        if len(self.image_list) == 0:
            raise ValueError(f"éæ¿¾å¾Œæ²’æœ‰æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ï¼")
        
        print(f"æ‰¾åˆ° {len(self.image_list)} å€‹æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ï¼ˆåœ¨ {self.root_dir}ï¼‰")
        self.watermark_bits = watermark_bits

    def __len__(self):
        return len(self.image_list)

    def _ensure_size(self, tensor):
        if tensor.shape[-2:] != TARGET_IMAGE_SIZE:
            tensor = F.interpolate(
                tensor.unsqueeze(0), size=TARGET_IMAGE_SIZE, mode='bilinear', align_corners=False
            ).squeeze(0)
        return tensor

    def __getitem__(self, idx):
        max_retries = 10
        
        for attempt in range(max_retries):
            try:
                current_idx = idx if attempt == 0 else random.randint(0, len(self.image_list) - 1)
                img_file = self.image_list[current_idx]
                
                if not isinstance(img_file, str):
                    img_file = str(img_file) if img_file is not None else ""
                
                if not img_file:
                    continue
                
                if os.path.isabs(img_file):
                    img_path = img_file
                else:
                    img_path = os.path.join(self.root_dir, img_file)
                
                if not isinstance(img_path, str):
                    img_path = str(img_path)
                
                image = Image.open(img_path)
                image = image.convert('RGB')
                image.load()
                
                if image.size is None or image.size[0] <= 0 or image.size[1] <= 0:
                    raise ValueError(f"ç„¡æ•ˆçš„åœ–ç‰‡å°ºå¯¸")
                
                image_tensor = self.transform(image)
                image.close()
                del image
                image_tensor = self._ensure_size(image_tensor)
                image_tensor = image_tensor.clone()
                
                watermark = torch.randint(0, 2, (self.watermark_bits,)).float()
                return image_tensor, watermark
                
            except Exception:
                continue
        
        fallback_image = Image.new('RGB', TARGET_IMAGE_SIZE, color=(0, 0, 0))
        image_tensor = self.transform(fallback_image)
        fallback_image = None
        image_tensor = self._ensure_size(image_tensor)
        watermark = torch.randint(0, 2, (self.watermark_bits,)).float()
        return image_tensor.clone(), watermark


# ============================================================
# Training Functionï¼ˆæ”¹é€²ç‰ˆï¼‰
# ============================================================
def train_model(epochs=100, batch_size=16, lr=None, device='cuda', 
                save_dir='./checkpoints_merged', use_vgg=True, resume_from_checkpoint=None,
                data_dir=None, watermark_bits=64, channels=64):
    """
    è¨“ç·´ ARWGAN åˆä½µæ¨¡å‹
    
    Args:
        epochs: è¨“ç·´ epochs
        batch_size: Batch size
        device: 'cuda' or 'cpu'
        save_dir: Checkpoint ä¿å­˜ç›®éŒ„
        use_vgg: æ˜¯å¦ä½¿ç”¨ VGG æ„ŸçŸ¥æå¤±
        resume_from_checkpoint: å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´
        data_dir: æ•¸æ“šé›†ç›®éŒ„
        watermark_bits: æµ®æ°´å°ä½æ•¸ï¼ˆé»˜èª 64ï¼‰
        channels: æ¨¡å‹é€šé“æ•¸ï¼ˆé»˜èª 64ï¼‰
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(exist_ok=True, parents=True)
    
    train_csv_path = save_dir / 'train.csv'
    validation_csv_path = save_dir / 'validation.csv'
    
    # è‡ªå‹•æª¢æ¸¬æ•¸æ“šé›†è·¯å¾‘
    if data_dir is None:
        possible_paths = [
            './data/coco2017/train/images',
            './data/coco/images/train2017',
            './data/train',
            './data/coco/train',
        ]
        data_dir = None
        for path in possible_paths:
            if os.path.exists(path) and os.path.isdir(path):
                try:
                    files = [f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                    if len(files) > 0:
                        data_dir = path
                        print(f"è‡ªå‹•æª¢æ¸¬åˆ°æ•¸æ“šé›†è·¯å¾‘: {data_dir} (æ‰¾åˆ° {len(files)} å€‹åœ–ç‰‡æ–‡ä»¶)")
                        break
                except (OSError, PermissionError):
                    continue
        
        if data_dir is None:
            raise ValueError(f"ç„¡æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ•¸æ“šé›†ï¼è«‹ä½¿ç”¨ --data-dir åƒæ•¸æŒ‡å®šæ•¸æ“šé›†è·¯å¾‘ã€‚")
    else:
        if not os.path.exists(data_dir):
            raise ValueError(f"æŒ‡å®šçš„æ•¸æ“šé›†è·¯å¾‘ä¸å­˜åœ¨: {data_dir}")
        print(f"ä½¿ç”¨æŒ‡å®šçš„æ•¸æ“šé›†è·¯å¾‘: {data_dir}")
    
    # è³‡æ–™é›†
    dataset = WatermarkDataset(root_dir=data_dir, watermark_bits=watermark_bits)
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    num_workers = 4
    pin_memory = torch.cuda.is_available()
    use_persistent_workers = num_workers > 0
    mp_context = multiprocessing.get_context('fork') if num_workers > 0 else None
    
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=pin_memory,
        persistent_workers=use_persistent_workers,
        prefetch_factor=2 if num_workers > 0 else None,
        multiprocessing_context=mp_context,
    )
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=pin_memory,
        persistent_workers=use_persistent_workers,
        prefetch_factor=2 if num_workers > 0 else None,
        multiprocessing_context=mp_context,
    )
    print(f"DataLoader é…ç½®: num_workers={num_workers}, pin_memory={pin_memory}, multiprocessing_context=fork")

    # éšæ®µå¸¸æ•¸ï¼ˆèˆ‡ NoiseLayer.warmup_epochs ä¸€è‡´ï¼Œé¿å…é‡è¤‡å®šç¾©ï¼‰
    NOISE_WARMUP_EPOCHS = 5
    GAN_WARMUP_EPOCHS = 15

    # ============================================================
    # æ¨¡å‹åˆå§‹åŒ–ï¼ˆä½¿ç”¨åŸå§‹ ARWGAN æ¶æ§‹ï¼‰
    # ============================================================
    print(f"\nåˆå§‹åŒ– ARWGAN åŸå§‹æ¶æ§‹æ¨¡å‹...")
    print(f"  - watermark_bits: {watermark_bits}")
    print(f"  - channels: {channels}")
    
    encoder = Encoder(watermark_bits=watermark_bits, channels=channels).to(device)
    noise_layer = NoiseLayer(device, warmup_epochs=NOISE_WARMUP_EPOCHS).to(device)
    decoder = Decoder(watermark_bits=watermark_bits, channels=channels).to(device)
    discriminator = Discriminator(watermark_bits=watermark_bits, channels=channels).to(device)
    
    # è¨ˆç®—æ¨¡å‹åƒæ•¸é‡
    enc_params = sum(p.numel() for p in encoder.parameters())
    dec_params = sum(p.numel() for p in decoder.parameters())
    disc_params = sum(p.numel() for p in discriminator.parameters())
    print(f"  - Encoder åƒæ•¸é‡: {enc_params:,}")
    print(f"  - Decoder åƒæ•¸é‡: {dec_params:,}")
    print(f"  - Discriminator åƒæ•¸é‡: {disc_params:,}")
    print(f"  - ç¸½åƒæ•¸é‡: {enc_params + dec_params + disc_params:,}\n")
    
    # VGG Loss
    vgg_loss_fn = VGGLoss().to(device) if use_vgg else None

    # å„ªåŒ–å™¨ï¼ˆå·®åˆ†å­¸ç¿’ç‡ï¼‰
    opt_gen = optim.Adam([
        {'params': encoder.parameters(), 'lr': 1e-4},
        {'params': decoder.parameters(), 'lr': 1e-3}
    ], betas=(0.5, 0.999))
    opt_disc = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler_gen = optim.lr_scheduler.StepLR(opt_gen, step_size=30, gamma=0.5)
    scheduler_disc = optim.lr_scheduler.StepLR(opt_disc, step_size=30, gamma=0.5)
    
    # æ··åˆç²¾åº¦è¨“ç·´ (AMP) - é™ä½é¡¯å­˜ä½¿ç”¨ï¼Œå…è¨±ä½¿ç”¨æ›´å¤§çš„ batch size
    scaler_gen = GradScaler()
    scaler_disc = GradScaler()
    
    # æå¤±å‡½æ•¸
    mse_loss = nn.MSELoss()
    bce_loss = nn.BCEWithLogitsLoss()
    
    # å¾æª¢æŸ¥é»æ¢å¾©
    start_epoch = 0
    best_val_ber = float('inf')
    patience_counter = 0

    if resume_from_checkpoint is not None and Path(resume_from_checkpoint).exists():
        try:
            print(f"\nå¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´: {resume_from_checkpoint}")
            checkpoint = torch.load(resume_from_checkpoint, map_location=device, weights_only=False)
            
            required_keys = ['encoder_state_dict', 'decoder_state_dict', 'discriminator_state_dict', 'epoch']
            missing_keys = [k for k in required_keys if k not in checkpoint]
            if missing_keys:
                raise KeyError(f"Checkpoint ç¼ºå°‘å¿…è¦çš„éµ: {missing_keys}")
            
            encoder.load_state_dict(checkpoint['encoder_state_dict'], strict=False)
            decoder.load_state_dict(checkpoint['decoder_state_dict'], strict=False)
            discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=False)
            
            if 'opt_gen_state_dict' in checkpoint:
                try:
                    opt_gen.load_state_dict(checkpoint['opt_gen_state_dict'])
                except Exception as e:
                    print(f"âš ï¸  å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥å¤±æ•—ï¼Œå°‡é‡æ–°åˆå§‹åŒ–: {e}")
            if 'opt_disc_state_dict' in checkpoint:
                try:
                    opt_disc.load_state_dict(checkpoint['opt_disc_state_dict'])
                except Exception as e:
                    print(f"âš ï¸  Discriminator å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥å¤±æ•—: {e}")
            
            if 'scheduler_gen_state_dict' in checkpoint:
                try:
                    scheduler_gen.load_state_dict(checkpoint['scheduler_gen_state_dict'])
                except Exception:
                    pass
            if 'scheduler_disc_state_dict' in checkpoint:
                try:
                    scheduler_disc.load_state_dict(checkpoint['scheduler_disc_state_dict'])
                except Exception:
                    pass
            
            # æ¢å¾©æ··åˆç²¾åº¦ scaler ç‹€æ…‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            # æ³¨æ„ï¼šèˆŠçš„æª¢æŸ¥é»å¯èƒ½æ²’æœ‰ scaler ç‹€æ…‹ï¼ˆAMP æ˜¯å¾Œä¾†åŠ å…¥çš„ï¼‰ï¼Œé€™æ˜¯æ­£å¸¸çš„
            scaler_loaded = False
            if 'scaler_gen_state_dict' in checkpoint:
                try:
                    scaler_gen.load_state_dict(checkpoint['scaler_gen_state_dict'])
                    scaler_loaded = True
                except Exception:
                    print(f"âš ï¸  Generator scaler ç‹€æ…‹è¼‰å…¥å¤±æ•—ï¼Œå°‡é‡æ–°åˆå§‹åŒ–")
            if 'scaler_disc_state_dict' in checkpoint:
                try:
                    scaler_disc.load_state_dict(checkpoint['scaler_disc_state_dict'])
                    scaler_loaded = True
                except Exception:
                    print(f"âš ï¸  Discriminator scaler ç‹€æ…‹è¼‰å…¥å¤±æ•—ï¼Œå°‡é‡æ–°åˆå§‹åŒ–")
            
            if not scaler_loaded:
                print(f"â„¹ï¸  æª¢æŸ¥é»ä¸­æ²’æœ‰ AMP scaler ç‹€æ…‹ï¼ˆå¯èƒ½æ˜¯èˆŠç‰ˆæœ¬ä¿å­˜çš„ï¼‰")
                print(f"   GradScaler å°‡å¾åˆå§‹ç‹€æ…‹é–‹å§‹ï¼Œæœƒè‡ªå‹•é©æ‡‰è¨“ç·´éç¨‹")
            
            start_epoch = checkpoint['epoch'] + 1
            patience_counter = checkpoint.get('patience_counter', 0)

            # Unleash Strategy: å¦‚æœå¾ Epoch >= 30 æ¢å¾©ï¼Œé‡ç½®æœ€ä½³ç´€éŒ„
            # å› ç‚ºé‡‹æ”¾æœŸçš„ç›®æ¨™ï¼ˆé«˜ PSNRï¼‰èˆ‡å‰æœŸä¸åŒï¼Œä¸æ‡‰èˆ‡ Phase 2 çš„æœ€ä½³ BER æ¯”è¼ƒ
            UNLEASH_EPOCH = 30
            if start_epoch >= UNLEASH_EPOCH:
                print(f"ğŸš€ æª¢æ¸¬åˆ°é‡‹æ”¾æœŸï¼ˆEpoch >= {UNLEASH_EPOCH}ï¼‰ï¼Œé‡ç½®æœ€ä½³ç´€éŒ„ä»¥é©æ‡‰æ–°çš„æ¬Šé‡ç­–ç•¥")
                best_val_ber = float('inf')
                patience_counter = 0
                print(f"   - best_val_ber å·²é‡ç½®ç‚º inf")
                print(f"   - patience_counter å·²é‡ç½®ç‚º 0")
            elif 'best_val_ber' in checkpoint:
                best_val_ber = checkpoint['best_val_ber']
            # patience_counter å·²æ–¼ä¸Šæ–¹ç”± checkpoint.get('patience_counter', 0) è¼‰å…¥

            print(f"âœ“ å·²æ¢å¾©åˆ° Epoch {start_epoch}")
            ber_str = f"{best_val_ber:.4f}" if best_val_ber != float('inf') else "inf (å·²é‡ç½®)"
            print(f"âœ“ æœ€ä½³é©—è­‰ BER: {ber_str}\n")
            
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥ checkpoint å¤±æ•—: {e}")
            print("   å°‡å¾é ­é–‹å§‹è¨“ç·´...\n")
            start_epoch = 0
            best_val_ber = float('inf')
            patience_counter = 0
            
    elif resume_from_checkpoint is not None:
        print(f"âš ï¸  è­¦å‘Š: æª¢æŸ¥é»æ–‡ä»¶ä¸å­˜åœ¨: {resume_from_checkpoint}")
        print("   å°‡å¾é ­é–‹å§‹è¨“ç·´...\n")
    
    print(f"é–‹å§‹è¨“ç·´... è¨“ç·´é›†: {train_size}, é©—è­‰é›†: {val_size}")
    if start_epoch > 0:
        print(f"å¾ Epoch {start_epoch} ç¹¼çºŒè¨“ç·´ï¼Œç¸½å…± {epochs} epochs\n")

    # ============================================================
    # éšæ®µå¼ Warm-up è¨­å®šï¼ˆNOISE_WARMUP_EPOCHS, GAN_WARMUP_EPOCHS å·²æ–¼ä¸Šæ–¹å®šç¾©ï¼‰
    # ============================================================
    early_stopping_patience = 15
    if start_epoch == 0:
        patience_counter = 0
    # è‹¥å¾ checkpoint æ¢å¾©ï¼Œpatience_counter å·²åœ¨ resume å€å¡Šå¾ checkpoint è¼‰å…¥
    UNLEASH_EPOCH = 30  # é‡‹æ”¾æœŸèµ·å§‹ epochï¼ˆresume æ™‚è‹¥ start_epoch >= 30 å·²åœ¨ä¸Šé¢é‡ç½® best_val_berï¼‰

    for epoch in range(start_epoch, epochs):
        noise_layer.set_epoch(epoch)
        gan_enabled = (epoch >= GAN_WARMUP_EPOCHS)
        
        # é¡¯ç¤ºç•¶å‰è¨“ç·´éšæ®µ
        if epoch < NOISE_WARMUP_EPOCHS:
            phase_name = "Phase 1: ç´”é€šè¨Šç³»çµ±"
            print(f"ğŸ”¥ {phase_name} (Epoch {epoch+1}/{NOISE_WARMUP_EPOCHS}): ç„¡ Noise, ç„¡ GAN")
        elif epoch < GAN_WARMUP_EPOCHS:
            phase_name = "Phase 2: æŠ—æ”»æ“Šè¨“ç·´"
            if epoch == NOISE_WARMUP_EPOCHS:
                print(f"\n{'='*60}")
                print(f"âœ… Phase 1 å®Œæˆï¼å¾ Epoch {epoch + 1} é–‹å§‹å•Ÿç”¨ Noise Layer æ”»æ“Š")
                print(f"{'='*60}\n")
            print(f"ğŸ”¥ {phase_name} (Epoch {epoch+1}): æœ‰ Noise, ç„¡ GAN")
        else:
            phase_name = "Phase 3: å®Œæ•´è¨“ç·´"
            if epoch == GAN_WARMUP_EPOCHS:
                print(f"\n{'='*60}")
                print(f"âœ… Phase 2 å®Œæˆï¼å¾ Epoch {epoch + 1} é–‹å§‹å•Ÿç”¨ GAN")
                print(f"{'='*60}\n")
                # æ¸…ç† GPU å¿«å–ï¼Œç‚º GAN è¨“ç·´é¨°å‡ºç©ºé–“
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    print("ğŸ§¹ å·²æ¸…ç† GPU å¿«å–")
        
        # ============= è¨“ç·´éšæ®µ =============
        encoder.train()
        decoder.train()
        discriminator.train()
        
        epoch_start_time = time.time()
        train_losses = {'g_loss': 0, 'd_loss': 0, 'ber': 0, 'psnr': 0}
        num_batches = 0
        
        for batch_idx, (images, watermarks) in enumerate(train_loader):
            images, watermarks = images.to(device), watermarks.to(device)
            
            # GAN è¨“ç·´æ™‚å®šæœŸæ¸…ç†å¿«å–ï¼ˆæ¯ 50 batchï¼‰
            if gan_enabled and batch_idx % 50 == 0:
                torch.cuda.empty_cache()
            
            # Train Discriminator (WGAN-GP)
            # æ³¨æ„ï¼šæ¢¯åº¦æ‡²ç½°ä¸ä½¿ç”¨ autocastï¼Œé¿å…æ··åˆç²¾åº¦é€ æˆæ•¸å€¼å•é¡Œ
            if gan_enabled:
                for _ in range(1):
                    opt_disc.zero_grad()
                    with autocast():
                        watermarked = encoder(images, watermarks)
                        d_real = discriminator(images)
                        d_fake = discriminator(watermarked.detach())
                    
                    # æ¢¯åº¦æ‡²ç½°åœ¨ autocast å¤–è¨ˆç®—ï¼ˆæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ + æ•¸å€¼ç©©å®šï¼‰
                    # ç¢ºä¿ä½¿ç”¨ .float() é¿å…æ··åˆç²¾åº¦å•é¡Œï¼Œä¸¦ç¢ºä¿åœ¨åŒä¸€è¨­å‚™ä¸Š
                    gp = wgan_gp_loss(
                        discriminator, 
                        images.float().detach(), 
                        watermarked.detach().float()
                    )
                    d_loss = -d_real + d_fake + gp
                    
                    scaler_disc.scale(d_loss).backward()
                    scaler_disc.step(opt_disc)
                    scaler_disc.update()
                    
                    # é‡‹æ”¾ Discriminator è¨“ç·´çš„ä¸­é–“è®Šæ•¸ï¼Œé¿å…è¨˜æ†¶é«”ç´¯ç©
                    del d_real, d_fake, gp, watermarked
            else:
                d_loss = torch.tensor(0.0, device=device)
            
            # Train Generator - ä½¿ç”¨æ··åˆç²¾åº¦
            opt_gen.zero_grad()
            with autocast():
                watermarked = encoder(images, watermarks)
                noised = noise_layer(watermarked, original_image=images)
                extracted, logits = decoder(noised)
                
                # Losses
                mse_img_loss = mse_loss(watermarked, images)
                ssim_img_loss = ssim_loss(watermarked, images)
                wm_loss = bce_loss(logits, watermarks)
                
                if gan_enabled:
                    g_gan_loss = -discriminator(watermarked)
                else:
                    g_gan_loss = torch.tensor(0.0, device=device)
                
                if vgg_loss_fn is not None:
                    vgg_real = vgg_loss_fn(images)
                    vgg_fake = vgg_loss_fn(watermarked)
                    vgg_perceptual_loss = mse_loss(vgg_fake, vgg_real)
                    img_loss = 0.5 * mse_img_loss + 0.3 * ssim_img_loss + 0.2 * vgg_perceptual_loss
                else:
                    img_loss = mse_img_loss + ssim_img_loss
                
                # æå¤±æ¬Šé‡æ’ç¨‹ (Unleash Strategy + Plan B + Golden Balance)
                if gan_enabled:
                    if epoch < 30:
                        # é«˜å£“æœŸï¼šåå‘æµ®æ°´å°
                        current_wm_weight = 8.0
                        current_img_weight = 1.0
                    elif epoch < 50:
                        # é‡‹æ”¾æœŸ Iï¼šwm é™æ¬Šï¼Œå°ˆæ³¨ç•«è³ª
                        current_wm_weight = 2.0
                        current_img_weight = 1.0
                    elif epoch < 52:
                        # Epoch 50â€“52: Plan B å¼·åˆ¶ç¾é¡æ¨¡å¼ï¼ˆBER çŠ§ç‰²æ› PSNRï¼‰
                        current_wm_weight = 1.0
                        current_img_weight = 5.0
                    else:
                        # Epoch 53+: é»ƒé‡‘å¹³è¡¡ (Golden Balance)
                        # ç¨å¾®æ‹‰å› BERï¼šæé«˜ wm æ¬Šé‡ã€é™ä½ç•«è³ªæ‡²ç½°
                        current_wm_weight = 4.0
                        current_img_weight = 2.0
                    
                    current_gan_weight = 0.001
                else:
                    # Phase 1-2: é™ä½ img æ¬Šé‡ã€æ‹‰é«˜ wmï¼Œè®“ Encoder æ•¢åµŒå…¥ã€Decoder èƒ½å­¸åˆ°ï¼Œç›®æ¨™ ber_clean < 0.1
                    current_img_weight = 0.3   # å…è¨±æ”¹åœ–ï¼Œå¦å‰‡ Encoder è¢«æ‡²ç½°ä¸æ•¢åµŒå…¥ï¼ŒBER å¡åœ¨ 0.5
                    current_wm_weight = 25.0    # å¼·è¿«å„ªå…ˆå„ªåŒ– BERï¼Œç„¡æ”»æ“Šæ™‚æ‡‰å¯é™åˆ° ~0.05
                    current_gan_weight = 0.0
                
                g_loss = current_img_weight * img_loss + current_wm_weight * wm_loss + current_gan_weight * g_gan_loss
            
            scaler_gen.scale(g_loss).backward()
            # æ¢¯åº¦è£å‰ªéœ€è¦åœ¨ scaler çš„ unscale ä¹‹å¾Œï¼Œä½† scaler.scale().backward() å¾Œå¯ä»¥ç›´æ¥ clip
            scaler_gen.unscale_(opt_gen)  # å…ˆ unscale æ‰èƒ½ clip
            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)
            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)
            scaler_gen.step(opt_gen)
            scaler_gen.update()
            
            with torch.no_grad():
                # extracted å·²ç¶“æ˜¯ 0/1 äºŒå€¼åŒ–çµæœï¼Œä¸éœ€è¦ .round()
                ber = (extracted != watermarks).float().mean().item()
                psnr = 10 * torch.log10(1.0 / mse_img_loss.clamp(min=1e-8)).item()
                
            train_losses['g_loss'] += g_loss.item()
            train_losses['d_loss'] += d_loss.item() if isinstance(d_loss, torch.Tensor) else d_loss
            train_losses['ber'] += ber
            train_losses['psnr'] += psnr
            num_batches += 1
            
            if batch_idx % 50 == 0:
                if gan_enabled:
                    phase_str = "Phase3"
                elif noise_layer.enable_attacks:
                    phase_str = "Phase2"
                else:
                    phase_str = "Phase1"
                print(f"[{phase_str}] Epoch [{epoch}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] "
                      f"G_loss: {g_loss.item():.4f}, D_loss: {d_loss.item() if isinstance(d_loss, torch.Tensor) else d_loss:.4f}, "
                      f"BER: {ber:.4f}, PSNR: {psnr:.2f}dB")
        
        if num_batches > 0:
            for key in train_losses:
                train_losses[key] /= num_batches
        
        train_duration = time.time() - epoch_start_time
        write_losses_to_csv(train_csv_path, train_losses, epoch + 1, train_duration)
        
        # ============= é©—è­‰éšæ®µ =============
        encoder.eval()
        decoder.eval()
        discriminator.eval()
        
        val_losses = {'ber': 0, 'ber_clean': 0, 'psnr': 0, 'ssim': 0}
        num_val_batches = 0
        
        with torch.no_grad():
            for images, watermarks in val_loader:
                images, watermarks = images.to(device), watermarks.to(device)
                
                watermarked = encoder(images, watermarks)
                noised = noise_layer(watermarked, original_image=images)
                extracted, _ = decoder(noised)
                extracted_clean, _ = decoder(watermarked)
                
                # extracted å’Œ extracted_clean å·²ç¶“æ˜¯ 0/1 äºŒå€¼åŒ–çµæœï¼Œä¸éœ€è¦ .round()
                ber = (extracted != watermarks).float().mean().item()
                ber_clean = (extracted_clean != watermarks).float().mean().item()
                mse = mse_loss(watermarked, images).item()
                psnr = 10 * np.log10(1.0 / max(mse, 1e-8))
                ssim_val = 1 - ssim_loss(watermarked, images).item()
                
                val_losses['ber'] += ber
                val_losses['ber_clean'] += ber_clean
                val_losses['psnr'] += psnr
                val_losses['ssim'] += ssim_val
                num_val_batches += 1
        
        if num_val_batches > 0:
            for key in val_losses:
                val_losses[key] /= num_val_batches
        
        total_duration = time.time() - epoch_start_time
        write_losses_to_csv(validation_csv_path, val_losses, epoch + 1, total_duration)
        
        print(f"\n{'='*80}")
        print(f"Epoch {epoch + 1}/{epochs} å®Œæˆ")
        print(f"è¨“ç·´ - G_loss: {train_losses['g_loss']:.4f}, BER: {train_losses['ber']:.4f}, PSNR: {train_losses['psnr']:.2f}dB")
        print(f"é©—è­‰ - BER(å«æ”»æ“Š): {val_losses['ber']:.4f}, BER(ç„¡æ”»æ“Š): {val_losses['ber_clean']:.4f}, PSNR: {val_losses['psnr']:.2f}dB, SSIM: {val_losses['ssim']:.4f}")
        print(f"{'='*80}\n")
        
        scheduler_gen.step()
        scheduler_disc.step()
        
        # Unleash Strategy: åœ¨é€²å…¥é‡‹æ”¾æœŸæ™‚é‡ç½®æœ€ä½³ç´€éŒ„
        if epoch == UNLEASH_EPOCH:
            print(f"\n{'='*60}")
            print(f"ğŸš€ é€²å…¥é‡‹æ”¾æœŸ (Epoch {epoch + 1})ï¼Œé‡ç½®æœ€ä½³ç´€éŒ„")
            old_ber_str = f"{best_val_ber:.4f}" if best_val_ber != float('inf') else "inf"
            print(f"   èˆŠæœ€ä½³ BER: {old_ber_str}")
            best_val_ber = float('inf')
            patience_counter = 0
            print(f"   æ–°æœ€ä½³ BER: inf (å·²é‡ç½®)")
            print(f"   patience_counter: 0 (å·²é‡ç½®)")
            print(f"{'='*60}\n")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_losses['ber'] < best_val_ber:
            best_val_ber = val_losses['ber']
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'encoder_state_dict': encoder.state_dict(),
                'decoder_state_dict': decoder.state_dict(),
                'discriminator_state_dict': discriminator.state_dict(),
                'opt_gen_state_dict': opt_gen.state_dict(),
                'opt_disc_state_dict': opt_disc.state_dict(),
                'best_val_ber': best_val_ber,
                'val_psnr': val_losses['psnr'],
                'val_ssim': val_losses['ssim'],
                'patience_counter': patience_counter,
            }, save_dir / 'best_model.pth')
            ber_str = f"{best_val_ber:.4f}" if best_val_ber != float('inf') else "inf"
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (BER: {ber_str})")
        else:
            patience_counter += 1
            print(f"â³ é©—è­‰ BER æœªæ”¹å–„ ({patience_counter}/{early_stopping_patience})")
        
        # æ¯å€‹ epoch ä¿å­˜ checkpointï¼ˆåŒ…å« scaler ç‹€æ…‹ï¼‰
        torch.save({
            'epoch': epoch,
            'encoder_state_dict': encoder.state_dict(),
            'decoder_state_dict': decoder.state_dict(),
            'discriminator_state_dict': discriminator.state_dict(),
            'opt_gen_state_dict': opt_gen.state_dict(),
            'opt_disc_state_dict': opt_disc.state_dict(),
            'scheduler_gen_state_dict': scheduler_gen.state_dict(),
            'scheduler_disc_state_dict': scheduler_disc.state_dict(),
            'scaler_gen_state_dict': scaler_gen.state_dict(),
            'scaler_disc_state_dict': scaler_disc.state_dict(),
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_val_ber': best_val_ber,
            'patience_counter': patience_counter,
        }, save_dir / f'checkpoint_epoch_{epoch}.pth')
        print(f"âœ“ ä¿å­˜æª¢æŸ¥é»: checkpoint_epoch_{epoch}.pth")
        
        # æ—©åœ
        if epoch >= GAN_WARMUP_EPOCHS and patience_counter >= early_stopping_patience:
            print(f"\n{'='*60}")
            print(f"ğŸ›‘ æ—©åœè§¸ç™¼ï¼šé©—è­‰ BER åœ¨ {early_stopping_patience} epochs å…§æœªæ”¹å–„")
            ber_str = f"{best_val_ber:.4f}" if best_val_ber != float('inf') else "inf"
            print(f"   æœ€ä½³ BER: {ber_str}")
            print(f"{'='*60}\n")
            break
    
    print("\nè¨“ç·´å®Œæˆï¼")
    return encoder, decoder, discriminator


# ============================================================
# Test Function
# ============================================================
def test_model(checkpoint_path, image_path, watermark_bits=64, channels=64, device='cuda', save_dir='./test_results'):
    save_dir = Path(save_dir)
    save_dir.mkdir(exist_ok=True, parents=True)
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    encoder = Encoder(watermark_bits, channels).to(device)
    decoder = Decoder(watermark_bits, channels).to(device)
    encoder.load_state_dict(checkpoint['encoder_state_dict'])
    decoder.load_state_dict(checkpoint['decoder_state_dict'])
    encoder.eval()
    decoder.eval()
    
    noise_layer = NoiseLayer(device).to(device)
    noise_layer.set_epoch(10)
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])
    image = transform(Image.open(image_path).convert('RGB')).unsqueeze(0).to(device)
    watermark = torch.randint(0, 2, (1, watermark_bits)).float().to(device)
    
    print(f"\n{'='*80}")
    print(f"æ¸¬è©¦åœ–åƒ: {image_path}")
    print(f"æ°´å°ä½æ•¸: {watermark_bits}")
    print(f"{'='*80}\n")
    
    with torch.no_grad():
        watermarked = encoder(image, watermark)
        
        mse = F.mse_loss(watermarked, image).item()
        psnr = 10 * np.log10(1.0 / max(mse, 1e-8))
        ssim_val = 1 - ssim_loss(watermarked, image).item()
        
        print(f"åŸå§‹åµŒå…¥å“è³ª:")
        print(f"  PSNR: {psnr:.2f} dB")
        print(f"  SSIM: {ssim_val:.4f}")
        print(f"  MSE:  {mse:.6f}\n")
        
        attacks = ['gaussian', 'jpeg', 'crop', 'dropout', 'resize']
        print(f"æ”»æ“Šé­¯æ£’æ€§æ¸¬è©¦:")
        print(f"{'-'*80}")
        
        for attack in attacks:
            noise_layer.attacks = [attack]
            noised = noise_layer(watermarked, original_image=image)
            extracted, _ = decoder(noised)
            # extracted å·²ç¶“æ˜¯ 0/1 äºŒå€¼åŒ–çµæœï¼Œä¸éœ€è¦ .round()
            ber = (extracted != watermark).float().mean().item()
            print(f"  {attack:15s}: BER = {ber:.4f} ({int(ber * watermark_bits)}/{watermark_bits} bits)")
        
        extracted_clean, _ = decoder(watermarked)
        # extracted_clean å·²ç¶“æ˜¯ 0/1 äºŒå€¼åŒ–çµæœï¼Œä¸éœ€è¦ .round()
        ber_clean = (extracted_clean != watermark).float().mean().item()
        print(f"  {'no_attack':15s}: BER = {ber_clean:.4f} ({int(ber_clean * watermark_bits)}/{watermark_bits} bits)")
        print(f"{'-'*80}\n")
        
        transforms.ToPILImage()(watermarked[0].cpu()).save(save_dir / 'watermarked.png')
        transforms.ToPILImage()(image[0].cpu()).save(save_dir / 'original.png')
        print(f"âœ“ çµæœå·²ä¿å­˜è‡³ {save_dir}")
        
        diff = torch.abs(watermarked - image) * 10
        transforms.ToPILImage()(diff[0].cpu()).save(save_dir / 'difference_x10.png')
        
    return {'psnr': psnr, 'ssim': ssim_val, 'ber_clean': ber_clean}


# ============================================================
# Main
# ============================================================
if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='ARWGAN åˆä½µç‰ˆæ°´å°æ¨¡å‹')
    parser.add_argument('--train', action='store_true', help='è¨“ç·´æ¨¡å¼')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼')
    parser.add_argument('--image', type=str, default='test.jpg', help='æ¸¬è©¦åœ–åƒè·¯å¾‘')
    parser.add_argument('--checkpoint', type=str, default='./checkpoints_merged/best_model.pth', help='checkpoint è·¯å¾‘ï¼ˆæ¸¬è©¦ç”¨ï¼‰')
    parser.add_argument('--resume', type=str, default=None, help='å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´ï¼ˆè¨“ç·´ç”¨ï¼‰')
    parser.add_argument('--epochs', type=int, default=100, help='è¨“ç·´ epochs')
    parser.add_argument('--batch', type=int, default=16, help='Batch sizeï¼ˆå·²å•Ÿç”¨æ··åˆç²¾åº¦ AMPï¼Œ24GB GPU å»ºè­° 16ï¼›è‹¥ä» OOM è«‹æ”¹ç”¨ 8ï¼‰')
    parser.add_argument('--use_vgg', action='store_true', help='ä½¿ç”¨ VGG æ„ŸçŸ¥æå¤±')
    parser.add_argument('--save_dir', type=str, default='./checkpoints_merged', help='æ¨¡å‹ä¿å­˜ç›®éŒ„')
    parser.add_argument('--data-dir', type=str, default=None, help='æ•¸æ“šé›†ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--watermark-bits', type=int, default=64, help='æµ®æ°´å°ä½æ•¸')
    parser.add_argument('--channels', type=int, default=64, help='æ¨¡å‹é€šé“æ•¸')
    args = parser.parse_args()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    if args.train:
        print("\n" + "="*60)
        print("ARWGAN åˆä½µç‰ˆ - åŸå§‹æ¶æ§‹ + æ”¹é€²è¨“ç·´æ¡†æ¶")
        print("="*60 + "\n")
        train_model(
            epochs=args.epochs, 
            batch_size=args.batch, 
            device=device,
            save_dir=args.save_dir,
            use_vgg=args.use_vgg,
            resume_from_checkpoint=args.resume,
            data_dir=args.data_dir,
            watermark_bits=args.watermark_bits,
            channels=args.channels
        )
    
    if args.test:
        print("\né–‹å§‹æ¸¬è©¦æ¨¡å‹...")
        if not Path(args.checkpoint).exists():
            print(f"éŒ¯èª¤: checkpoint ä¸å­˜åœ¨: {args.checkpoint}")
        else:
            test_model(
                checkpoint_path=args.checkpoint,
                image_path=args.image,
                watermark_bits=args.watermark_bits,
                channels=args.channels,
                device=device
            )
